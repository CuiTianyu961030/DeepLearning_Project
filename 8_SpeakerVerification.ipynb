{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=6><bold>实验七、声纹识别</bold></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按一定比例划分训练集与数据集并采用滑动窗口方法利用librosa语音处理库对语音的MFCC（梅尔频率倒谱系数）特征进行提取，并保存为.npy格式的文件至指定文件夹，文件表示每一位说话者每一条口语的声纹特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow as  tf\n",
    "import time\n",
    "from tensorflow.contrib import rnn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义全局配置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG(object):\n",
    "    def __init__(self):\n",
    "        self.sr = 8000\n",
    "        self.nfft = 512\n",
    "        self.window = 0.025\n",
    "        self.hop = 0.01\n",
    "        self.tisv_frame = 180\n",
    "        self.train_path = \"./train_tisv\"\n",
    "        self.test_path = \"./test_tisv\"\n",
    "        \n",
    "        self.hidden = 128\n",
    "        self.proj = 64\n",
    "        self.num_layer = 3\n",
    "        self.restore = False\n",
    "        self.model_path = \"./model\"\n",
    "        self.model_num = 5\n",
    "        \n",
    "        self.train = False\n",
    "        self.N = 8\n",
    "        self.M = 10\n",
    "        self.loss = \"softmax\"\n",
    "        self.optim = \"sgd\"\n",
    "        self.lr = 1e-2\n",
    "        self.beta1 = 0.5\n",
    "        self.beta2 = 0.9\n",
    "        self.iteration = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG()\n",
    "audio_path = \"./wav48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrogram_tisv():\n",
    "    print(\"start text independent utterance feature extraction\")\n",
    "    os.makedirs(config.train_path, exist_ok=True)   # make folder to save train file\n",
    "    os.makedirs(config.test_path, exist_ok=True)    # make folder to save test file\n",
    "\n",
    "    utter_min_len = (config.tisv_frame * config.hop + config.window) * config.sr    # lower bound of utterance length\n",
    "    total_speaker_num = len(os.listdir(audio_path))\n",
    "    train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "    print(\"total speaker number : %d\"%total_speaker_num)\n",
    "    print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n",
    "    for i, folder in enumerate(os.listdir(audio_path)):\n",
    "        if i <= 107:\n",
    "            continue\n",
    "        speaker_path = os.path.join(audio_path, folder)     # path of each speaker\n",
    "        print(\"%dth speaker processing...\"%i)\n",
    "        utterances_spec = []\n",
    "        for utter_name in os.listdir(speaker_path):\n",
    "            utter_path = os.path.join(speaker_path, utter_name)         # path of each utterance\n",
    "            utter, sr = librosa.core.load(utter_path, config.sr)        # load utterance audio\n",
    "            intervals = librosa.effects.split(utter, top_db=20)         # voice activity detection\n",
    "            for interval in intervals:\n",
    "                if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                    utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "                    S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                                          win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "                    S = np.abs(S) ** 2\n",
    "                    mel_basis = librosa.filters.mel(sr=config.sr, n_fft=config.nfft, n_mels=40)\n",
    "                    S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "\n",
    "                    utterances_spec.append(S[:, :config.tisv_frame])    # first 180 frames of partial utterance\n",
    "                    utterances_spec.append(S[:, -config.tisv_frame:])   # last 180 frames of partial utterance\n",
    "\n",
    "        utterances_spec = np.array(utterances_spec)\n",
    "        print(utterances_spec.shape)\n",
    "        if i<train_speaker_num:      # save spectrogram as numpy file\n",
    "            np.save(os.path.join(config.train_path, \"speaker%d.npy\"%i), utterances_spec)\n",
    "        else:\n",
    "            np.save(os.path.join(config.test_path, \"speaker%d.npy\"%(i-train_speaker_num)), utterances_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start text independent utterance feature extraction\n",
      "total speaker number : 109\n",
      "train : 90, test : 19\n",
      "108th speaker processing...\n",
      "(260, 40, 180)\n"
     ]
    }
   ],
   "source": [
    "save_spectrogram_tisv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、构建训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、在步骤一中生成的.npy文件中，随机选择N位说话者并对应选择M条口语声纹特征，组成的batch大小N×M。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(speaker_num=config.N, utter_num=config.M, shuffle=True, noise_filenum=None, utter_start=0):\n",
    "    # data path\n",
    "    if config.train:\n",
    "        path = config.train_path\n",
    "    else:\n",
    "        path = config.test_path\n",
    "\n",
    "    np_file_list = os.listdir(path)\n",
    "    total_speaker = len(np_file_list)\n",
    "\n",
    "    if shuffle:\n",
    "        selected_files = random.sample(np_file_list, speaker_num)  # select random N speakers\n",
    "    else:\n",
    "        selected_files = np_file_list[:speaker_num]                # select first N speakers\n",
    "\n",
    "    utter_batch = []\n",
    "    for file in selected_files:\n",
    "        utters = np.load(os.path.join(path, file))        # load utterance spectrogram of selected speaker\n",
    "        if shuffle:\n",
    "            utter_index = np.random.randint(0, utters.shape[0], utter_num)   # select M utterances per speaker\n",
    "            utter_batch.append(utters[utter_index])       # each speakers utterance [M, n_mels, frames] is appended\n",
    "        else:\n",
    "            utter_batch.append(utters[utter_start: utter_start+utter_num])\n",
    "\n",
    "    utter_batch = np.concatenate(utter_batch, axis=0)     # utterance batch [batch(NM), n_mels, frames]\n",
    "\n",
    "    if config.train:\n",
    "        frame_slice = np.random.randint(140,181)          # for train session, random slicing of input batch\n",
    "        utter_batch = utter_batch[:,:,:frame_slice]\n",
    "    else:\n",
    "        utter_batch = utter_batch[:,:,:160]               # for train session, fixed length slicing of input batch\n",
    "\n",
    "    utter_batch = np.transpose(utter_batch, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "    return utter_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、定义数据归一化函数和相似度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\" normalize the last dimension vector of the input matrix\n",
    "    :return: normalized input\n",
    "    \"\"\"\n",
    "    return x/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keepdims=True)+1e-6)\n",
    "\n",
    "\n",
    "def cossim(x,y, normalized=True):\n",
    "    \"\"\" calculate similarity between tensors\n",
    "    :return: cos similarity tf op node\n",
    "    \"\"\"\n",
    "    if normalized:\n",
    "        return tf.reduce_sum(x*y)\n",
    "    else:\n",
    "        x_norm = tf.sqrt(tf.reduce_sum(x**2)+1e-6)\n",
    "        y_norm = tf.sqrt(tf.reduce_sum(y**2)+1e-6)\n",
    "        return tf.reduce_sum(x*y)/x_norm/y_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、计算相似度矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![相似度](./图片2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(embedded, w, b, N=config.N, M=config.M, P=config.proj, center=None):\n",
    "    \"\"\" Calculate similarity matrix from embedded utterance batch (NM x embed_dim) eq. (9)\n",
    "        Input center to test enrollment. (embedded for verification)\n",
    "    :return: tf similarity matrix (NM x N)\n",
    "    \"\"\"\n",
    "    embedded_split = tf.reshape(embedded, shape=[N, M, P])\n",
    "\n",
    "    if center is None:\n",
    "        center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n",
    "        center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keepdims=True)\n",
    "                                             - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n",
    "        # make similarity matrix eq.(9)\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keepdims=True) if i==j\n",
    "                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keepdims=True) for i in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "    else :\n",
    "        # If center(enrollment) exist, use it.\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keepdims=True) for i\n",
    "                        in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "\n",
    "    S = tf.abs(w)*S+b   # rescaling\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、根据相似度矩阵计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal(S, type=\"softmax\", N=config.N, M=config.M):\n",
    "    \"\"\" calculate loss with similarity matrix(S) eq.(6) (7) \n",
    "    :type: \"softmax\" or \"contrast\"\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    S_correct = tf.concat([S[i*M:(i+1)*M, i:(i+1)] for i in range(N)], axis=0)  # colored entries in Fig.1\n",
    "\n",
    "    if type == \"softmax\":\n",
    "        total = -tf.reduce_sum(S_correct-tf.log(tf.reduce_sum(tf.exp(S), axis=1, keepdims=True) + 1e-6))\n",
    "    elif type == \"contrast\":\n",
    "        S_sig = tf.sigmoid(S)\n",
    "        S_sig = tf.concat([tf.concat([0*S_sig[i*M:(i+1)*M, j:(j+1)] if i==j\n",
    "                              else S_sig[i*M:(i+1)*M, j:(j+1)] for j in range(N)], axis=1)\n",
    "                             for i in range(N)], axis=0)\n",
    "        total = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keepdims=True))\n",
    "    else:\n",
    "        raise AssertionError(\"loss type should be softmax or contrast !\")\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、定义模型优化器函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim(lr):\n",
    "    \"\"\" return optimizer determined by configuration\n",
    "    :return: tf optimizer\n",
    "    \"\"\"\n",
    "    if config.optim == \"sgd\":\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    elif config.optim == \"rmsprop\":\n",
    "        return tf.train.RMSPropOptimizer(lr)\n",
    "    elif config.optim == \"adam\":\n",
    "        return tf.train.AdamOptimizer(lr, beta1=config.beta1, beta2=config.beta2)\n",
    "    else:\n",
    "        raise AssertionError(\"Wrong optimizer type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、定义训练模型函数\n",
    "\n",
    "本实验使用了3层LSTM网络，其输出作为Embedding d-Vector，再进行L2正则化，得到的向量就是说话人的声纹表征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    tf.reset_default_graph()    # reset graph\n",
    "\n",
    "    # draw graph\n",
    "    batch = tf.placeholder(shape= [None, config.N*config.M, 40], dtype=tf.float32)  # input batch (time x batch x n_mel)\n",
    "    lr = tf.placeholder(dtype= tf.float32)  # learning rate\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    w = tf.get_variable(\"w\", initializer= np.array([10], dtype=np.float32))\n",
    "    b = tf.get_variable(\"b\", initializer= np.array([-5], dtype=np.float32))\n",
    "\n",
    "    # embedding lstm (3-layer default)\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # define lstm op and variables\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "        embedded = normalize(embedded)                    # normalize\n",
    "    print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "    # loss\n",
    "    sim_matrix = similarity(embedded, w, b)\n",
    "    print(\"similarity matrix size: \", sim_matrix.shape)\n",
    "    loss = loss_cal(sim_matrix, type=config.loss)\n",
    "\n",
    "    # optimizer operation\n",
    "    trainable_vars= tf.trainable_variables()                # get variable list\n",
    "    optimizer= optim(lr)                                    # get optimizer (type is determined by configuration)\n",
    "    grads, vars= zip(*optimizer.compute_gradients(loss))    # compute gradients of variables with respect to loss\n",
    "    grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)      # l2 norm clipping by 3\n",
    "    grads_rescale= [0.01*grad for grad in grads_clip[:2]] + grads_clip[2:]   # smaller gradient scale for w, b\n",
    "    train_op= optimizer.apply_gradients(zip(grads_rescale, vars), global_step= global_step)   # gradient update operation\n",
    "\n",
    "    # check variables memory\n",
    "    variable_count = np.sum(np.array([np.prod(np.array(v.get_shape().as_list())) for v in trainable_vars]))\n",
    "    print(\"total variables :\", variable_count)\n",
    "\n",
    "    # record loss\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # training session\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n",
    "        os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n",
    "        writer = tf.summary.FileWriter(os.path.join(path, \"logs\"), sess.graph)\n",
    "        epoch = 0\n",
    "        lr_factor = 1   # lr decay factor ( 1/2 per 10000 iteration)\n",
    "        loss_acc = 0    # accumulated loss ( for running average of loss)\n",
    "\n",
    "        for iter in range(config.iteration):\n",
    "            # run forward and backward propagation and update parameters\n",
    "            _, loss_cur, summary = sess.run([train_op, loss, merged],\n",
    "                                  feed_dict={batch: random_batch(), lr: config.lr*lr_factor})\n",
    "\n",
    "            loss_acc += loss_cur    # accumulated loss for each 100 iteration\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                writer.add_summary(summary, iter)   # write at tensorboard\n",
    "            if (iter+1) % 100 == 0:\n",
    "                print(\"(iter : %d) loss: %.4f\" % ((iter+1),loss_acc/100))\n",
    "                loss_acc = 0                        # reset accumulated loss\n",
    "            if (iter+1) % 10000 == 0:\n",
    "                lr_factor /= 2                      # lr decay\n",
    "                print(\"learning rate is decayed! current lr : \", config.lr*lr_factor)\n",
    "            if (iter+1) % 10000 == 0:\n",
    "                saver.save(sess, os.path.join(path, \"./Check_Point/model.ckpt\"), global_step=iter//10000)\n",
    "                print(\"model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7、定义测试模型函数\n",
    "\n",
    "利用已处理好的测试数据进行模型的测试使用，并根据评价指标EER分析实验结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Session\n",
    "def test(path):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # draw graph\n",
    "    enroll = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32) # enrollment batch (time x batch x n_mel)\n",
    "    verif = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "    batch = tf.concat([enroll, verif], axis=1)\n",
    "\n",
    "    # embedding lstm (3-layer default)\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "        embedded = normalize(embedded)                    # normalize\n",
    "\n",
    "    print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "    # enrollment embedded vectors (speaker model)\n",
    "    enroll_embed = normalize(tf.reduce_mean(tf.reshape(embedded[:config.N*config.M, :], shape= [config.N, config.M, -1]), axis=1))\n",
    "    # verification embedded vectors\n",
    "    verif_embed = embedded[config.N*config.M:, :]\n",
    "\n",
    "    similarity_matrix = similarity(embedded=verif_embed, w=1., b=0., center=enroll_embed)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # load model\n",
    "        print(\"model path :\", path)\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=os.path.join(path, \"Check_Point\"))\n",
    "        ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "        loaded = 0\n",
    "        for model in ckpt_list:\n",
    "            if config.model_num == int(model[-1]):    # find ckpt file which matches configuration model number\n",
    "                print(\"ckpt file is loaded !\", model)\n",
    "                loaded = 1\n",
    "                saver.restore(sess, model)  # restore variables from selected ckpt file\n",
    "                break\n",
    "\n",
    "        if loaded == 0:\n",
    "            raise AssertionError(\"ckpt file does not exist! Check config.model_num or config.model_path.\")\n",
    "\n",
    "        print(\"test file path : \", config.test_path)\n",
    "\n",
    "        # return similarity matrix after enrollment and verification\n",
    "        time1 = time.time() # for check inference time\n",
    "        S = sess.run(similarity_matrix, feed_dict={enroll:random_batch(shuffle=False),\n",
    "                                                       verif:random_batch(shuffle=False, utter_start=config.M)})\n",
    "        S = S.reshape([config.N, config.M, -1])\n",
    "        time2 = time.time()\n",
    "\n",
    "        np.set_printoptions(precision=2)\n",
    "        print(\"inference time for %d utterences : %0.2fs\"%(2*config.M*config.N, time2-time1))\n",
    "        print(S)    # print similarity matrix\n",
    "\n",
    "        # calculating EER\n",
    "        diff = 1; EER=0; EER_thres = 0; EER_FAR=0; EER_FRR=0\n",
    "\n",
    "        # through thresholds calculate false acceptance ratio (FAR) and false reject ratio (FRR)\n",
    "        for thres in [0.01*i+0.5 for i in range(50)]:\n",
    "            S_thres = S>thres\n",
    "\n",
    "            # False acceptance ratio = false acceptance / mismatched population (enroll speaker != verification speaker)\n",
    "            FAR = sum([np.sum(S_thres[i])-np.sum(S_thres[i,:,i]) for i in range(config.N)])/(config.N-1)/config.M/config.N\n",
    "\n",
    "            # False reject ratio = false reject / matched population (enroll speaker = verification speaker)\n",
    "            FRR = sum([config.M-np.sum(S_thres[i][:,i]) for i in range(config.N)])/config.M/config.N\n",
    "\n",
    "            # Save threshold when FAR = FRR (=EER)\n",
    "            if diff> abs(FAR-FRR):\n",
    "                diff = abs(FAR-FRR)\n",
    "                EER = (FAR+FRR)/2\n",
    "                EER_thres = thres\n",
    "                EER_FAR = FAR\n",
    "                EER_FRR = FRR\n",
    "\n",
    "        print(\"\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)\"%(EER,EER_thres,EER_FAR,EER_FRR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded size:  (80, 64)\n",
      "similarity matrix size:  (80, 8)\n",
      "total variables : 210434\n",
      "(iter : 100) loss: 125.1124\n",
      "(iter : 200) loss: 102.2868\n",
      "(iter : 300) loss: 89.9621\n",
      "(iter : 400) loss: 86.9429\n",
      "(iter : 500) loss: 79.9499\n",
      "(iter : 600) loss: 75.2657\n",
      "(iter : 700) loss: 70.0881\n",
      "(iter : 800) loss: 65.6341\n",
      "(iter : 900) loss: 62.9165\n",
      "(iter : 1000) loss: 61.5927\n",
      "(iter : 1100) loss: 54.8214\n",
      "(iter : 1200) loss: 51.2984\n",
      "(iter : 1300) loss: 48.3105\n",
      "(iter : 1400) loss: 45.9122\n",
      "(iter : 1500) loss: 41.6896\n",
      "(iter : 1600) loss: 43.2339\n",
      "(iter : 1700) loss: 38.5662\n",
      "(iter : 1800) loss: 36.0750\n",
      "(iter : 1900) loss: 35.9866\n",
      "(iter : 2000) loss: 35.6972\n",
      "(iter : 2100) loss: 32.1313\n",
      "(iter : 2200) loss: 32.2583\n",
      "(iter : 2300) loss: 31.1465\n",
      "(iter : 2400) loss: 28.6453\n",
      "(iter : 2500) loss: 29.0325\n",
      "(iter : 2600) loss: 28.3069\n",
      "(iter : 2700) loss: 27.3037\n",
      "(iter : 2800) loss: 26.8961\n",
      "(iter : 2900) loss: 26.5261\n",
      "(iter : 3000) loss: 27.4743\n",
      "(iter : 3100) loss: 26.0053\n",
      "(iter : 3200) loss: 25.8477\n",
      "(iter : 3300) loss: 24.0968\n",
      "(iter : 3400) loss: 25.1196\n",
      "(iter : 3500) loss: 23.1311\n",
      "(iter : 3600) loss: 23.4995\n",
      "(iter : 3700) loss: 23.4513\n",
      "(iter : 3800) loss: 21.4265\n",
      "(iter : 3900) loss: 23.1132\n",
      "(iter : 4000) loss: 22.9096\n",
      "(iter : 4100) loss: 20.7972\n",
      "(iter : 4200) loss: 20.9216\n",
      "(iter : 4300) loss: 22.6000\n",
      "(iter : 4400) loss: 21.3948\n",
      "(iter : 4500) loss: 20.1084\n",
      "(iter : 4600) loss: 19.0182\n",
      "(iter : 4700) loss: 20.5251\n",
      "(iter : 4800) loss: 20.6271\n",
      "(iter : 4900) loss: 20.0620\n",
      "(iter : 5000) loss: 18.2316\n",
      "(iter : 5100) loss: 19.5083\n",
      "(iter : 5200) loss: 17.8354\n",
      "(iter : 5300) loss: 19.2086\n",
      "(iter : 5400) loss: 18.4276\n",
      "(iter : 5500) loss: 18.0853\n",
      "(iter : 5600) loss: 16.8695\n",
      "(iter : 5700) loss: 16.1810\n",
      "(iter : 5800) loss: 17.3694\n",
      "(iter : 5900) loss: 17.9695\n",
      "(iter : 6000) loss: 18.7489\n",
      "(iter : 6100) loss: 14.8636\n",
      "(iter : 6200) loss: 16.2349\n",
      "(iter : 6300) loss: 17.2699\n",
      "(iter : 6400) loss: 15.9706\n",
      "(iter : 6500) loss: 17.6411\n",
      "(iter : 6600) loss: 15.8970\n",
      "(iter : 6700) loss: 16.7262\n",
      "(iter : 6800) loss: 16.3059\n",
      "(iter : 6900) loss: 16.4714\n",
      "(iter : 7000) loss: 15.3530\n",
      "(iter : 7100) loss: 14.8779\n",
      "(iter : 7200) loss: 15.7628\n",
      "(iter : 7300) loss: 15.7174\n",
      "(iter : 7400) loss: 15.3888\n",
      "(iter : 7500) loss: 15.8906\n",
      "(iter : 7600) loss: 14.1748\n",
      "(iter : 7700) loss: 14.7313\n",
      "(iter : 7800) loss: 15.0035\n",
      "(iter : 7900) loss: 15.3389\n",
      "(iter : 8000) loss: 14.7931\n",
      "(iter : 8100) loss: 14.1820\n",
      "(iter : 8200) loss: 14.6710\n",
      "(iter : 8300) loss: 14.4680\n",
      "(iter : 8400) loss: 12.9227\n",
      "(iter : 8500) loss: 14.4477\n",
      "(iter : 8600) loss: 14.0131\n",
      "(iter : 8700) loss: 14.1133\n",
      "(iter : 8800) loss: 12.9073\n",
      "(iter : 8900) loss: 14.6619\n",
      "(iter : 9000) loss: 13.1309\n",
      "(iter : 9100) loss: 12.8601\n",
      "(iter : 9200) loss: 13.0869\n",
      "(iter : 9300) loss: 13.3302\n",
      "(iter : 9400) loss: 12.3137\n",
      "(iter : 9500) loss: 13.0615\n",
      "(iter : 9600) loss: 13.2034\n",
      "(iter : 9700) loss: 13.0240\n",
      "(iter : 9800) loss: 12.0275\n",
      "(iter : 9900) loss: 12.6227\n",
      "(iter : 10000) loss: 13.8931\n",
      "learning rate is decayed! current lr :  0.005\n",
      "model is saved!\n",
      "(iter : 10100) loss: 11.3030\n",
      "(iter : 10200) loss: 10.1317\n",
      "(iter : 10300) loss: 11.6815\n",
      "(iter : 10400) loss: 10.6280\n",
      "(iter : 10500) loss: 11.5178\n",
      "(iter : 10600) loss: 10.2910\n",
      "(iter : 10700) loss: 10.0618\n",
      "(iter : 10800) loss: 10.4116\n",
      "(iter : 10900) loss: 11.0984\n",
      "(iter : 11000) loss: 10.8993\n",
      "(iter : 11100) loss: 10.2046\n",
      "(iter : 11200) loss: 10.3189\n",
      "(iter : 11300) loss: 10.6825\n",
      "(iter : 11400) loss: 10.2830\n",
      "(iter : 11500) loss: 10.5885\n",
      "(iter : 11600) loss: 9.9892\n",
      "(iter : 11700) loss: 10.4700\n",
      "(iter : 11800) loss: 9.3798\n",
      "(iter : 11900) loss: 9.9150\n",
      "(iter : 12000) loss: 10.9176\n",
      "(iter : 12100) loss: 10.1597\n",
      "(iter : 12200) loss: 9.9589\n",
      "(iter : 12300) loss: 9.4352\n",
      "(iter : 12400) loss: 10.8907\n",
      "(iter : 12500) loss: 10.8410\n",
      "(iter : 12600) loss: 10.9076\n",
      "(iter : 12700) loss: 9.6158\n",
      "(iter : 12800) loss: 9.6700\n",
      "(iter : 12900) loss: 9.3035\n",
      "(iter : 13000) loss: 9.9638\n",
      "(iter : 13100) loss: 8.7488\n",
      "(iter : 13200) loss: 9.7100\n",
      "(iter : 13300) loss: 10.0202\n",
      "(iter : 13400) loss: 9.0262\n",
      "(iter : 13500) loss: 10.2791\n",
      "(iter : 13600) loss: 9.8496\n",
      "(iter : 13700) loss: 9.5234\n",
      "(iter : 13800) loss: 10.0217\n",
      "(iter : 13900) loss: 9.4878\n",
      "(iter : 14000) loss: 9.5881\n",
      "(iter : 14100) loss: 10.0928\n",
      "(iter : 14200) loss: 8.4435\n",
      "(iter : 14300) loss: 9.4998\n",
      "(iter : 14400) loss: 9.3261\n",
      "(iter : 14500) loss: 9.4715\n",
      "(iter : 14600) loss: 9.6004\n",
      "(iter : 14700) loss: 9.3574\n",
      "(iter : 14800) loss: 9.0400\n",
      "(iter : 14900) loss: 9.4266\n",
      "(iter : 15000) loss: 9.6147\n",
      "(iter : 15100) loss: 8.1049\n",
      "(iter : 15200) loss: 10.1856\n",
      "(iter : 15300) loss: 9.2970\n",
      "(iter : 15400) loss: 9.4341\n",
      "(iter : 15500) loss: 9.9012\n",
      "(iter : 15600) loss: 8.7888\n",
      "(iter : 15700) loss: 8.5273\n",
      "(iter : 15800) loss: 9.0859\n",
      "(iter : 15900) loss: 8.5088\n",
      "(iter : 16000) loss: 8.1488\n",
      "(iter : 16100) loss: 9.0367\n",
      "(iter : 16200) loss: 8.8542\n",
      "(iter : 16300) loss: 8.4030\n",
      "(iter : 16400) loss: 9.2126\n",
      "(iter : 16500) loss: 8.8424\n",
      "(iter : 16600) loss: 8.6824\n",
      "(iter : 16700) loss: 8.3212\n",
      "(iter : 16800) loss: 9.3472\n",
      "(iter : 16900) loss: 8.7888\n",
      "(iter : 17000) loss: 8.5208\n",
      "(iter : 17100) loss: 8.3289\n",
      "(iter : 17200) loss: 9.2287\n",
      "(iter : 17300) loss: 8.5320\n",
      "(iter : 17400) loss: 8.5493\n",
      "(iter : 17500) loss: 8.8745\n",
      "(iter : 17600) loss: 9.2978\n",
      "(iter : 17700) loss: 8.4141\n",
      "(iter : 17800) loss: 9.4525\n",
      "(iter : 17900) loss: 9.1388\n",
      "(iter : 18000) loss: 9.3942\n",
      "(iter : 18100) loss: 9.0028\n",
      "(iter : 18200) loss: 8.0412\n",
      "(iter : 18300) loss: 8.2772\n",
      "(iter : 18400) loss: 7.6248\n",
      "(iter : 18500) loss: 7.7044\n",
      "(iter : 18600) loss: 7.8588\n",
      "(iter : 18700) loss: 8.2669\n",
      "(iter : 18800) loss: 7.7532\n",
      "(iter : 18900) loss: 7.6978\n",
      "(iter : 19000) loss: 8.0033\n",
      "(iter : 19100) loss: 7.9617\n",
      "(iter : 19200) loss: 8.1638\n",
      "(iter : 19300) loss: 7.7100\n",
      "(iter : 19400) loss: 8.4997\n",
      "(iter : 19500) loss: 8.7061\n",
      "(iter : 19600) loss: 8.2648\n",
      "(iter : 19700) loss: 8.5872\n",
      "(iter : 19800) loss: 7.9430\n",
      "(iter : 19900) loss: 8.0119\n",
      "(iter : 20000) loss: 7.6592\n",
      "learning rate is decayed! current lr :  0.0025\n",
      "model is saved!\n",
      "(iter : 20100) loss: 6.8616\n",
      "(iter : 20200) loss: 6.9169\n",
      "(iter : 20300) loss: 7.8132\n",
      "(iter : 20400) loss: 7.8254\n",
      "(iter : 20500) loss: 8.3232\n",
      "(iter : 20600) loss: 7.5797\n",
      "(iter : 20700) loss: 8.0273\n",
      "(iter : 20800) loss: 7.6046\n",
      "(iter : 20900) loss: 7.9540\n",
      "(iter : 21000) loss: 6.8477\n",
      "(iter : 21100) loss: 8.0044\n",
      "(iter : 21200) loss: 7.5676\n",
      "(iter : 21300) loss: 6.9838\n",
      "(iter : 21400) loss: 6.5275\n",
      "(iter : 21500) loss: 7.9485\n",
      "(iter : 21600) loss: 7.6059\n",
      "(iter : 21700) loss: 6.6720\n",
      "(iter : 21800) loss: 7.0466\n",
      "(iter : 21900) loss: 6.2690\n",
      "(iter : 22000) loss: 7.7404\n",
      "(iter : 22100) loss: 6.5648\n",
      "(iter : 22200) loss: 7.0133\n",
      "(iter : 22300) loss: 6.2991\n",
      "(iter : 22400) loss: 6.8944\n",
      "(iter : 22500) loss: 6.3249\n",
      "(iter : 22600) loss: 6.8914\n",
      "(iter : 22700) loss: 7.6698\n",
      "(iter : 22800) loss: 7.3043\n",
      "(iter : 22900) loss: 6.7923\n",
      "(iter : 23000) loss: 7.0147\n",
      "(iter : 23100) loss: 7.2050\n",
      "(iter : 23200) loss: 7.9540\n",
      "(iter : 23300) loss: 7.0974\n",
      "(iter : 23400) loss: 7.0630\n",
      "(iter : 23500) loss: 7.2091\n",
      "(iter : 23600) loss: 6.3066\n",
      "(iter : 23700) loss: 7.4328\n",
      "(iter : 23800) loss: 6.1453\n",
      "(iter : 23900) loss: 7.0220\n",
      "(iter : 24000) loss: 6.9030\n",
      "(iter : 24100) loss: 6.6377\n",
      "(iter : 24200) loss: 7.4818\n",
      "(iter : 24300) loss: 7.2563\n",
      "(iter : 24400) loss: 6.8584\n",
      "(iter : 24500) loss: 6.4224\n",
      "(iter : 24600) loss: 7.2655\n",
      "(iter : 24700) loss: 6.3695\n",
      "(iter : 24800) loss: 6.5579\n",
      "(iter : 24900) loss: 6.4468\n",
      "(iter : 25000) loss: 6.4815\n",
      "(iter : 25100) loss: 6.7222\n",
      "(iter : 25200) loss: 6.3811\n",
      "(iter : 25300) loss: 6.1798\n",
      "(iter : 25400) loss: 6.6120\n",
      "(iter : 25500) loss: 6.6860\n",
      "(iter : 25600) loss: 7.2635\n",
      "(iter : 25700) loss: 6.8251\n",
      "(iter : 25800) loss: 6.5754\n",
      "(iter : 25900) loss: 6.8174\n",
      "(iter : 26000) loss: 7.7046\n",
      "(iter : 26100) loss: 7.2207\n",
      "(iter : 26200) loss: 7.0398\n",
      "(iter : 26300) loss: 6.5856\n",
      "(iter : 26400) loss: 6.7113\n",
      "(iter : 26500) loss: 6.9175\n",
      "(iter : 26600) loss: 6.8487\n",
      "(iter : 26700) loss: 6.1153\n",
      "(iter : 26800) loss: 7.0726\n",
      "(iter : 26900) loss: 6.4806\n",
      "(iter : 27000) loss: 7.1133\n",
      "(iter : 27100) loss: 6.1659\n",
      "(iter : 27200) loss: 7.2581\n",
      "(iter : 27300) loss: 6.2285\n",
      "(iter : 27400) loss: 7.5509\n",
      "(iter : 27500) loss: 7.0549\n",
      "(iter : 27600) loss: 5.8054\n",
      "(iter : 27700) loss: 6.9110\n",
      "(iter : 27800) loss: 6.9032\n",
      "(iter : 27900) loss: 7.0663\n",
      "(iter : 28000) loss: 6.7402\n",
      "(iter : 28100) loss: 6.7317\n",
      "(iter : 28200) loss: 6.8238\n",
      "(iter : 28300) loss: 6.0625\n",
      "(iter : 28400) loss: 6.4912\n",
      "(iter : 28500) loss: 6.2927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iter : 28600) loss: 6.0508\n",
      "(iter : 28700) loss: 5.8554\n",
      "(iter : 28800) loss: 6.1459\n",
      "(iter : 28900) loss: 6.6405\n",
      "(iter : 29000) loss: 6.1738\n",
      "(iter : 29100) loss: 6.7811\n",
      "(iter : 29200) loss: 6.3596\n",
      "(iter : 29300) loss: 6.6660\n",
      "(iter : 29400) loss: 7.1217\n",
      "(iter : 29500) loss: 7.2853\n",
      "(iter : 29600) loss: 6.0356\n",
      "(iter : 29700) loss: 6.9032\n",
      "(iter : 29800) loss: 6.5223\n",
      "(iter : 29900) loss: 6.9550\n",
      "(iter : 30000) loss: 6.4661\n",
      "learning rate is decayed! current lr :  0.00125\n",
      "model is saved!\n",
      "(iter : 30100) loss: 6.4794\n",
      "(iter : 30200) loss: 6.0957\n",
      "(iter : 30300) loss: 5.8880\n",
      "(iter : 30400) loss: 6.8376\n",
      "(iter : 30500) loss: 5.5698\n",
      "(iter : 30600) loss: 6.2785\n",
      "(iter : 30700) loss: 5.2932\n",
      "(iter : 30800) loss: 5.7432\n",
      "(iter : 30900) loss: 5.4160\n",
      "(iter : 31000) loss: 6.5425\n",
      "(iter : 31100) loss: 5.7405\n",
      "(iter : 31200) loss: 6.9640\n",
      "(iter : 31300) loss: 5.5323\n",
      "(iter : 31400) loss: 6.5483\n",
      "(iter : 31500) loss: 5.9443\n",
      "(iter : 31600) loss: 5.8195\n",
      "(iter : 31700) loss: 5.6073\n",
      "(iter : 31800) loss: 6.0223\n",
      "(iter : 31900) loss: 5.5022\n",
      "(iter : 32000) loss: 5.6353\n",
      "(iter : 32100) loss: 5.6601\n",
      "(iter : 32200) loss: 6.0250\n",
      "(iter : 32300) loss: 6.8285\n",
      "(iter : 32400) loss: 5.9613\n",
      "(iter : 32500) loss: 5.6028\n",
      "(iter : 32600) loss: 5.9611\n",
      "(iter : 32700) loss: 5.9681\n",
      "(iter : 32800) loss: 5.5568\n",
      "(iter : 32900) loss: 5.8729\n",
      "(iter : 33000) loss: 6.3844\n",
      "(iter : 33100) loss: 6.2450\n",
      "(iter : 33200) loss: 5.6581\n",
      "(iter : 33300) loss: 5.3461\n",
      "(iter : 33400) loss: 6.1448\n",
      "(iter : 33500) loss: 6.0203\n",
      "(iter : 33600) loss: 5.2881\n",
      "(iter : 33700) loss: 5.5329\n",
      "(iter : 33800) loss: 6.1572\n",
      "(iter : 33900) loss: 5.5910\n",
      "(iter : 34000) loss: 5.3045\n",
      "(iter : 34100) loss: 5.7411\n",
      "(iter : 34200) loss: 6.0294\n",
      "(iter : 34300) loss: 5.7551\n",
      "(iter : 34400) loss: 5.7276\n",
      "(iter : 34500) loss: 5.2570\n",
      "(iter : 34600) loss: 6.4097\n",
      "(iter : 34700) loss: 5.8104\n",
      "(iter : 34800) loss: 5.6107\n",
      "(iter : 34900) loss: 5.5763\n",
      "(iter : 35000) loss: 5.9881\n",
      "(iter : 35100) loss: 5.3598\n",
      "(iter : 35200) loss: 5.8363\n",
      "(iter : 35300) loss: 5.1685\n",
      "(iter : 35400) loss: 5.4901\n",
      "(iter : 35500) loss: 6.2269\n",
      "(iter : 35600) loss: 5.7540\n",
      "(iter : 35700) loss: 5.6384\n",
      "(iter : 35800) loss: 5.7954\n",
      "(iter : 35900) loss: 5.8762\n",
      "(iter : 36000) loss: 5.4596\n",
      "(iter : 36100) loss: 6.0190\n",
      "(iter : 36200) loss: 5.4797\n",
      "(iter : 36300) loss: 5.7611\n",
      "(iter : 36400) loss: 5.6628\n",
      "(iter : 36500) loss: 5.4364\n",
      "(iter : 36600) loss: 6.1101\n",
      "(iter : 36700) loss: 5.8566\n",
      "(iter : 36800) loss: 5.6815\n",
      "(iter : 36900) loss: 5.5675\n",
      "(iter : 37000) loss: 5.4405\n",
      "(iter : 37100) loss: 5.4371\n",
      "(iter : 37200) loss: 5.7139\n",
      "(iter : 37300) loss: 5.8147\n",
      "(iter : 37400) loss: 5.9531\n",
      "(iter : 37500) loss: 6.3149\n",
      "(iter : 37600) loss: 5.5864\n",
      "(iter : 37700) loss: 6.6413\n",
      "(iter : 37800) loss: 6.4545\n",
      "(iter : 37900) loss: 5.8501\n",
      "(iter : 38000) loss: 5.6393\n",
      "(iter : 38100) loss: 5.0896\n",
      "(iter : 38200) loss: 5.5581\n",
      "(iter : 38300) loss: 5.5386\n",
      "(iter : 38400) loss: 4.9809\n",
      "(iter : 38500) loss: 5.6937\n",
      "(iter : 38600) loss: 5.7875\n",
      "(iter : 38700) loss: 5.8045\n",
      "(iter : 38800) loss: 5.4540\n",
      "(iter : 38900) loss: 5.9911\n",
      "(iter : 39000) loss: 4.9759\n",
      "(iter : 39100) loss: 5.2068\n",
      "(iter : 39200) loss: 5.7893\n",
      "(iter : 39300) loss: 5.3823\n",
      "(iter : 39400) loss: 5.8179\n",
      "(iter : 39500) loss: 5.5376\n",
      "(iter : 39600) loss: 5.9500\n",
      "(iter : 39700) loss: 6.2621\n",
      "(iter : 39800) loss: 5.2691\n",
      "(iter : 39900) loss: 5.2749\n",
      "(iter : 40000) loss: 5.8379\n",
      "learning rate is decayed! current lr :  0.000625\n",
      "model is saved!\n",
      "(iter : 40100) loss: 5.6666\n",
      "(iter : 40200) loss: 5.0203\n",
      "(iter : 40300) loss: 5.4461\n",
      "(iter : 40400) loss: 5.4882\n",
      "(iter : 40500) loss: 5.2715\n",
      "(iter : 40600) loss: 5.5353\n",
      "(iter : 40700) loss: 5.6435\n",
      "(iter : 40800) loss: 5.1290\n",
      "(iter : 40900) loss: 5.0596\n",
      "(iter : 41000) loss: 5.4713\n",
      "(iter : 41100) loss: 5.5117\n",
      "(iter : 41200) loss: 6.0590\n",
      "(iter : 41300) loss: 5.1364\n",
      "(iter : 41400) loss: 5.3703\n",
      "(iter : 41500) loss: 4.9714\n",
      "(iter : 41600) loss: 4.8057\n",
      "(iter : 41700) loss: 4.8815\n",
      "(iter : 41800) loss: 5.2175\n",
      "(iter : 41900) loss: 5.3827\n",
      "(iter : 42000) loss: 4.7106\n",
      "(iter : 42100) loss: 5.8010\n",
      "(iter : 42200) loss: 5.1960\n",
      "(iter : 42300) loss: 5.0520\n",
      "(iter : 42400) loss: 5.1033\n",
      "(iter : 42500) loss: 4.5719\n",
      "(iter : 42600) loss: 5.4548\n",
      "(iter : 42700) loss: 5.4223\n",
      "(iter : 42800) loss: 5.7508\n",
      "(iter : 42900) loss: 5.3275\n",
      "(iter : 43000) loss: 5.2812\n",
      "(iter : 43100) loss: 5.2947\n",
      "(iter : 43200) loss: 5.1553\n",
      "(iter : 43300) loss: 5.3747\n",
      "(iter : 43400) loss: 5.8999\n",
      "(iter : 43500) loss: 5.3276\n",
      "(iter : 43600) loss: 5.7653\n",
      "(iter : 43700) loss: 4.8070\n",
      "(iter : 43800) loss: 5.3410\n",
      "(iter : 43900) loss: 5.5143\n",
      "(iter : 44000) loss: 5.6690\n",
      "(iter : 44100) loss: 4.6998\n",
      "(iter : 44200) loss: 5.7023\n",
      "(iter : 44300) loss: 5.8401\n",
      "(iter : 44400) loss: 4.8846\n",
      "(iter : 44500) loss: 5.1676\n",
      "(iter : 44600) loss: 5.4089\n",
      "(iter : 44700) loss: 6.0530\n",
      "(iter : 44800) loss: 4.8464\n",
      "(iter : 44900) loss: 5.5657\n",
      "(iter : 45000) loss: 5.3389\n",
      "(iter : 45100) loss: 5.1903\n",
      "(iter : 45200) loss: 5.0831\n",
      "(iter : 45300) loss: 5.2362\n",
      "(iter : 45400) loss: 5.2488\n",
      "(iter : 45500) loss: 5.3134\n",
      "(iter : 45600) loss: 5.2800\n",
      "(iter : 45700) loss: 5.1196\n",
      "(iter : 45800) loss: 5.2331\n",
      "(iter : 45900) loss: 5.3244\n",
      "(iter : 46000) loss: 5.9557\n",
      "(iter : 46100) loss: 5.0990\n",
      "(iter : 46200) loss: 5.6907\n",
      "(iter : 46300) loss: 5.3492\n",
      "(iter : 46400) loss: 5.7573\n",
      "(iter : 46500) loss: 5.1194\n",
      "(iter : 46600) loss: 5.0296\n",
      "(iter : 46700) loss: 4.8507\n",
      "(iter : 46800) loss: 5.4791\n",
      "(iter : 46900) loss: 5.0376\n",
      "(iter : 47000) loss: 4.7316\n",
      "(iter : 47100) loss: 5.4847\n",
      "(iter : 47200) loss: 4.7419\n",
      "(iter : 47300) loss: 5.1602\n",
      "(iter : 47400) loss: 5.2695\n",
      "(iter : 47500) loss: 5.6097\n",
      "(iter : 47600) loss: 4.5336\n",
      "(iter : 47700) loss: 5.4781\n",
      "(iter : 47800) loss: 5.0843\n",
      "(iter : 47900) loss: 5.5880\n",
      "(iter : 48000) loss: 5.1140\n",
      "(iter : 48100) loss: 4.8295\n",
      "(iter : 48200) loss: 5.1302\n",
      "(iter : 48300) loss: 5.5779\n",
      "(iter : 48400) loss: 5.8680\n",
      "(iter : 48500) loss: 4.8852\n",
      "(iter : 48600) loss: 5.5230\n",
      "(iter : 48700) loss: 4.8935\n",
      "(iter : 48800) loss: 5.3663\n",
      "(iter : 48900) loss: 4.9887\n",
      "(iter : 49000) loss: 5.0624\n",
      "(iter : 49100) loss: 5.4627\n",
      "(iter : 49200) loss: 5.4449\n",
      "(iter : 49300) loss: 4.5881\n",
      "(iter : 49400) loss: 5.1473\n",
      "(iter : 49500) loss: 4.9362\n",
      "(iter : 49600) loss: 5.2656\n",
      "(iter : 49700) loss: 5.3196\n",
      "(iter : 49800) loss: 5.7607\n",
      "(iter : 49900) loss: 5.1004\n",
      "(iter : 50000) loss: 5.0850\n",
      "learning rate is decayed! current lr :  0.0003125\n",
      "model is saved!\n",
      "(iter : 50100) loss: 5.0695\n",
      "(iter : 50200) loss: 5.2510\n",
      "(iter : 50300) loss: 5.0692\n",
      "(iter : 50400) loss: 5.0668\n",
      "(iter : 50500) loss: 4.9191\n",
      "(iter : 50600) loss: 5.3715\n",
      "(iter : 50700) loss: 5.5805\n",
      "(iter : 50800) loss: 4.9803\n",
      "(iter : 50900) loss: 4.9457\n",
      "(iter : 51000) loss: 4.5678\n",
      "(iter : 51100) loss: 5.1060\n",
      "(iter : 51200) loss: 5.0562\n",
      "(iter : 51300) loss: 5.0572\n",
      "(iter : 51400) loss: 5.3131\n",
      "(iter : 51500) loss: 4.9337\n",
      "(iter : 51600) loss: 5.6328\n",
      "(iter : 51700) loss: 5.1947\n",
      "(iter : 51800) loss: 5.0602\n",
      "(iter : 51900) loss: 5.5297\n",
      "(iter : 52000) loss: 4.9329\n",
      "(iter : 52100) loss: 4.7286\n",
      "(iter : 52200) loss: 4.8670\n",
      "(iter : 52300) loss: 4.8244\n",
      "(iter : 52400) loss: 5.6606\n",
      "(iter : 52500) loss: 4.9943\n",
      "(iter : 52600) loss: 5.1817\n",
      "(iter : 52700) loss: 4.9566\n",
      "(iter : 52800) loss: 4.5835\n",
      "(iter : 52900) loss: 4.8447\n",
      "(iter : 53000) loss: 4.4857\n",
      "(iter : 53100) loss: 4.7622\n",
      "(iter : 53200) loss: 4.8365\n",
      "(iter : 53300) loss: 5.0940\n",
      "(iter : 53400) loss: 5.5394\n",
      "(iter : 53500) loss: 5.0838\n",
      "(iter : 53600) loss: 5.1202\n",
      "(iter : 53700) loss: 4.6518\n",
      "(iter : 53800) loss: 4.9755\n",
      "(iter : 53900) loss: 4.8903\n",
      "(iter : 54000) loss: 4.5273\n",
      "(iter : 54100) loss: 4.8837\n",
      "(iter : 54200) loss: 5.8058\n",
      "(iter : 54300) loss: 5.0808\n",
      "(iter : 54400) loss: 5.2725\n",
      "(iter : 54500) loss: 4.8350\n",
      "(iter : 54600) loss: 5.5083\n",
      "(iter : 54700) loss: 4.7948\n",
      "(iter : 54800) loss: 5.1863\n",
      "(iter : 54900) loss: 5.6006\n",
      "(iter : 55000) loss: 4.6568\n",
      "(iter : 55100) loss: 5.0928\n",
      "(iter : 55200) loss: 4.9535\n",
      "(iter : 55300) loss: 5.2271\n",
      "(iter : 55400) loss: 6.0313\n",
      "(iter : 55500) loss: 5.6088\n",
      "(iter : 55600) loss: 5.1145\n",
      "(iter : 55700) loss: 5.1067\n",
      "(iter : 55800) loss: 5.0314\n",
      "(iter : 55900) loss: 4.4354\n",
      "(iter : 56000) loss: 5.3336\n",
      "(iter : 56100) loss: 4.6303\n",
      "(iter : 56200) loss: 5.4911\n",
      "(iter : 56300) loss: 4.8011\n",
      "(iter : 56400) loss: 4.7278\n",
      "(iter : 56500) loss: 4.3644\n",
      "(iter : 56600) loss: 4.8671\n",
      "(iter : 56700) loss: 4.1991\n",
      "(iter : 56800) loss: 5.0187\n",
      "(iter : 56900) loss: 4.8290\n",
      "(iter : 57000) loss: 5.2489\n",
      "(iter : 57100) loss: 4.8422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iter : 57200) loss: 4.7432\n",
      "(iter : 57300) loss: 5.2697\n",
      "(iter : 57400) loss: 4.3247\n",
      "(iter : 57500) loss: 5.0200\n",
      "(iter : 57600) loss: 4.9058\n",
      "(iter : 57700) loss: 4.7815\n",
      "(iter : 57800) loss: 5.0891\n",
      "(iter : 57900) loss: 5.6554\n",
      "(iter : 58000) loss: 4.7796\n",
      "(iter : 58100) loss: 4.6124\n",
      "(iter : 58200) loss: 4.3885\n",
      "(iter : 58300) loss: 5.4216\n",
      "(iter : 58400) loss: 5.0885\n",
      "(iter : 58500) loss: 5.1005\n",
      "(iter : 58600) loss: 4.5721\n",
      "(iter : 58700) loss: 4.9611\n",
      "(iter : 58800) loss: 5.1177\n",
      "(iter : 58900) loss: 5.3826\n",
      "(iter : 59000) loss: 5.0858\n",
      "(iter : 59100) loss: 5.3879\n",
      "(iter : 59200) loss: 5.5520\n",
      "(iter : 59300) loss: 4.6062\n",
      "(iter : 59400) loss: 4.9064\n",
      "(iter : 59500) loss: 5.0871\n",
      "(iter : 59600) loss: 4.8817\n",
      "(iter : 59700) loss: 5.1417\n",
      "(iter : 59800) loss: 5.0438\n",
      "(iter : 59900) loss: 5.2398\n",
      "(iter : 60000) loss: 4.5126\n",
      "learning rate is decayed! current lr :  0.00015625\n",
      "model is saved!\n"
     ]
    }
   ],
   "source": [
    "config.train= True\n",
    "train(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded size:  (160, 64)\n",
      "model path : ./model\n",
      "ckpt file is loaded ! ./model\\Check_Point\\model.ckpt-5\n",
      "INFO:tensorflow:Restoring parameters from ./model\\Check_Point\\model.ckpt-5\n",
      "test file path :  ./test_tisv\n",
      "inference time for 160 utterences : 0.59s\n",
      "[[[ 0.84  0.25 -0.21  0.33  0.12  0.25  0.49  0.11]\n",
      "  [ 0.85  0.53 -0.42  0.47  0.21  0.44  0.34 -0.02]\n",
      "  [ 0.88  0.41 -0.15  0.34  0.28  0.47  0.36  0.08]\n",
      "  [ 0.81  0.55 -0.15  0.43  0.22  0.45  0.22  0.07]\n",
      "  [ 0.74  0.08 -0.07  0.53  0.03  0.18  0.36  0.01]\n",
      "  [ 0.8   0.39  0.06 -0.06  0.32  0.33  0.51  0.21]\n",
      "  [ 0.79  0.35 -0.28  0.21  0.24  0.36  0.16  0.07]\n",
      "  [ 0.64  0.42  0.12 -0.12  0.33  0.47  0.21  0.21]\n",
      "  [ 0.36  0.18  0.19 -0.05  0.53  0.24 -0.08  0.07]\n",
      "  [ 0.68  0.59 -0.34  0.38  0.29  0.56 -0.01 -0.12]]\n",
      "\n",
      " [[ 0.34  0.79 -0.58  0.14 -0.12  0.69 -0.14 -0.44]\n",
      "  [ 0.28  0.74  0.14 -0.02  0.39  0.63 -0.19  0.  ]\n",
      "  [ 0.3   0.77 -0.34  0.29  0.    0.72 -0.22 -0.6 ]\n",
      "  [ 0.12  0.86 -0.16 -0.04  0.02  0.74 -0.19 -0.18]\n",
      "  [ 0.54  0.79 -0.49  0.4   0.03  0.72  0.04 -0.33]\n",
      "  [ 0.    0.7   0.08 -0.22 -0.    0.62 -0.19  0.09]\n",
      "  [ 0.6   0.91 -0.41  0.29  0.06  0.79 -0.04 -0.28]\n",
      "  [ 0.45  0.81 -0.22 -0.02  0.1   0.67  0.05  0.17]\n",
      "  [ 0.54  0.91 -0.18  0.11  0.16  0.84 -0.14 -0.11]\n",
      "  [ 0.43  0.75 -0.36  0.08  0.    0.68 -0.13  0.11]]\n",
      "\n",
      " [[-0.05 -0.55  0.84 -0.05  0.04 -0.45  0.22  0.47]\n",
      "  [ 0.01 -0.46  0.95 -0.14  0.11 -0.33  0.34  0.42]\n",
      "  [-0.07 -0.46  0.98 -0.02 -0.03 -0.41  0.34  0.39]\n",
      "  [-0.05 -0.38  0.99 -0.12  0.   -0.3   0.3   0.47]\n",
      "  [-0.12 -0.33  0.97 -0.11  0.04 -0.27  0.18  0.28]\n",
      "  [-0.08 -0.35  0.96 -0.05  0.02 -0.25  0.15  0.32]\n",
      "  [-0.11 -0.49  0.96 -0.09 -0.   -0.39  0.22  0.41]\n",
      "  [ 0.02 -0.46  0.94 -0.11  0.15 -0.36  0.36  0.41]\n",
      "  [ 0.18 -0.29  0.8  -0.03  0.02 -0.29  0.51  0.44]\n",
      "  [-0.11 -0.43  0.92 -0.24  0.03 -0.27  0.2   0.51]]\n",
      "\n",
      " [[ 0.33  0.31 -0.32  0.93 -0.4   0.19  0.26 -0.26]\n",
      "  [ 0.55  0.2   0.02  0.82 -0.07  0.11  0.31 -0.14]\n",
      "  [ 0.46  0.28 -0.26  0.94 -0.31  0.16  0.4  -0.17]\n",
      "  [ 0.34  0.24 -0.1   0.91 -0.23  0.07  0.28 -0.14]\n",
      "  [ 0.16  0.2  -0.06  0.86 -0.43  0.09  0.13 -0.28]\n",
      "  [ 0.48  0.07  0.06  0.93 -0.26  0.01  0.39  0.01]\n",
      "  [ 0.23 -0.04  0.07  0.9  -0.28 -0.08  0.21 -0.16]\n",
      "  [ 0.14  0.1   0.2   0.77 -0.13  0.05  0.04 -0.16]\n",
      "  [ 0.53  0.39 -0.43  0.87 -0.34  0.28  0.31 -0.2 ]\n",
      "  [ 0.5   0.37 -0.36  0.86 -0.43  0.28  0.31 -0.16]]\n",
      "\n",
      " [[ 0.22  0.18  0.05 -0.33  0.92  0.14 -0.07 -0.18]\n",
      "  [ 0.26  0.2   0.05 -0.42  0.92  0.17 -0.06 -0.05]\n",
      "  [ 0.29  0.03  0.15 -0.37  0.81  0.07  0.05 -0.12]\n",
      "  [ 0.19  0.15  0.15 -0.39  0.94  0.15 -0.07 -0.07]\n",
      "  [ 0.12 -0.05 -0.24 -0.29  0.89 -0.12 -0.06 -0.21]\n",
      "  [-0.03  0.08  0.07 -0.34  0.85  0.07 -0.21 -0.4 ]\n",
      "  [-0.    0.02 -0.04 -0.35  0.88 -0.   -0.17 -0.39]\n",
      "  [ 0.2   0.18  0.16 -0.31  0.88  0.13 -0.08 -0.11]\n",
      "  [ 0.09  0.05 -0.14 -0.32  0.94 -0.03 -0.15 -0.27]\n",
      "  [ 0.15  0.06 -0.11 -0.31  0.95 -0.03 -0.09 -0.2 ]]\n",
      "\n",
      " [[ 0.48  0.65 -0.59  0.33  0.03  0.61 -0.03 -0.32]\n",
      "  [ 0.1   0.8  -0.2  -0.18  0.03  0.88 -0.45 -0.29]\n",
      "  [ 0.41  0.76 -0.65  0.23 -0.07  0.72 -0.14 -0.44]\n",
      "  [ 0.52  0.73 -0.59  0.22 -0.    0.67  0.02 -0.33]\n",
      "  [-0.07  0.54  0.23 -0.11  0.1   0.51 -0.25 -0.32]\n",
      "  [ 0.2   0.69  0.19 -0.17  0.31  0.68 -0.26 -0.13]\n",
      "  [ 0.44  0.37 -0.28  0.64 -0.26  0.33 -0.01 -0.22]\n",
      "  [ 0.1   0.68 -0.   -0.25  0.12  0.75 -0.35 -0.13]\n",
      "  [ 0.3   0.39 -0.28  0.71 -0.34  0.44 -0.05 -0.11]\n",
      "  [ 0.02  0.3   0.05 -0.25  0.2   0.5  -0.4   0.16]]\n",
      "\n",
      " [[ 0.46 -0.36  0.21  0.35  0.01 -0.51  0.89  0.26]\n",
      "  [ 0.4  -0.12  0.51 -0.05  0.2  -0.26  0.83  0.42]\n",
      "  [ 0.2  -0.4   0.31  0.24  0.07 -0.58  0.81  0.26]\n",
      "  [ 0.47 -0.03  0.36  0.22  0.03 -0.21  0.91  0.35]\n",
      "  [ 0.43 -0.39  0.32  0.44 -0.11 -0.46  0.9   0.23]\n",
      "  [ 0.46 -0.13  0.33  0.33  0.09 -0.28  0.92  0.23]\n",
      "  [ 0.51 -0.19  0.11  0.37  0.05 -0.28  0.92  0.2 ]\n",
      "  [ 0.38 -0.15  0.36  0.2   0.08 -0.3   0.86  0.33]\n",
      "  [ 0.25 -0.12  0.08  0.35 -0.04 -0.33  0.77  0.1 ]\n",
      "  [ 0.5  -0.08  0.23  0.3   0.03 -0.22  0.92  0.22]]\n",
      "\n",
      " [[ 0.07 -0.1   0.41  0.08 -0.39 -0.18  0.18  0.86]\n",
      "  [ 0.08 -0.12  0.55 -0.31  0.08 -0.14  0.27  0.89]\n",
      "  [ 0.24 -0.09  0.18 -0.02 -0.13 -0.11  0.16  0.86]\n",
      "  [ 0.23 -0.16  0.27 -0.04 -0.17 -0.16  0.24  0.9 ]\n",
      "  [ 0.2  -0.08  0.47 -0.37  0.15 -0.14  0.42  0.76]\n",
      "  [ 0.12 -0.38  0.56 -0.24  0.2  -0.42  0.43  0.78]\n",
      "  [-0.09 -0.32  0.2   0.15 -0.31 -0.32 -0.04  0.73]\n",
      "  [ 0.14 -0.15  0.58 -0.35  0.23 -0.15  0.29  0.79]\n",
      "  [ 0.01 -0.18  0.45  0.03 -0.43 -0.18  0.26  0.74]\n",
      "  [ 0.27 -0.12  0.54 -0.17  0.11 -0.21  0.53  0.87]]]\n",
      "\n",
      "EER : 0.06 (thres:0.52, FAR:0.06, FRR:0.06)\n"
     ]
    }
   ],
   "source": [
    "config.train = False\n",
    "test(\"./model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
