{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-382bf6c0fe3b>:34: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-13-382bf6c0fe3b>:38: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-13-382bf6c0fe3b>:52: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/cuitianyu/工程/JupyterProject/3_NNLM/reader.py:111: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-382bf6c0fe3b>:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "iteration: 1\n",
      "After 0 steps, perplexity is 9991.693\n",
      "After 100 steps, perplexity is 1368.135\n",
      "After 200 steps, perplexity is 1016.444\n",
      "After 300 steps, perplexity is 861.139\n",
      "After 400 steps, perplexity is 758.693\n",
      "After 500 steps, perplexity is 687.294\n",
      "After 600 steps, perplexity is 635.404\n",
      "After 700 steps, perplexity is 590.937\n",
      "After 800 steps, perplexity is 551.342\n",
      "After 900 steps, perplexity is 520.570\n",
      "After 1000 steps, perplexity is 496.984\n",
      "After 1100 steps, perplexity is 474.271\n",
      "After 1200 steps, perplexity is 455.221\n",
      "After 1300 steps, perplexity is 437.944\n",
      "After 1400 steps, perplexity is 424.717\n",
      "After 1500 steps, perplexity is 411.449\n",
      "After 1600 steps, perplexity is 401.825\n",
      "After 1700 steps, perplexity is 391.512\n",
      "After 1800 steps, perplexity is 382.468\n",
      "After 1900 steps, perplexity is 374.342\n",
      "After 2000 steps, perplexity is 367.741\n",
      "After 2100 steps, perplexity is 359.635\n",
      "After 2200 steps, perplexity is 352.457\n",
      "After 2300 steps, perplexity is 347.055\n",
      "After 2400 steps, perplexity is 340.689\n",
      "After 2500 steps, perplexity is 335.053\n",
      "After 2600 steps, perplexity is 328.525\n",
      "After 2700 steps, perplexity is 324.858\n",
      "After 2800 steps, perplexity is 319.449\n",
      "After 2900 steps, perplexity is 315.964\n",
      "After 3000 steps, perplexity is 312.237\n",
      "After 3100 steps, perplexity is 308.559\n",
      "After 3200 steps, perplexity is 305.521\n",
      "After 3300 steps, perplexity is 302.761\n",
      "After 3400 steps, perplexity is 299.094\n",
      "After 3500 steps, perplexity is 295.664\n",
      "After 3600 steps, perplexity is 292.868\n",
      "After 3700 steps, perplexity is 290.207\n",
      "After 3800 steps, perplexity is 287.238\n",
      "After 3900 steps, perplexity is 283.833\n",
      "After 4000 steps, perplexity is 281.712\n",
      "After 4100 steps, perplexity is 278.771\n",
      "After 4200 steps, perplexity is 276.887\n",
      "After 4300 steps, perplexity is 274.716\n",
      "After 4400 steps, perplexity is 272.784\n",
      "After 4500 steps, perplexity is 270.944\n",
      "After 4600 steps, perplexity is 269.400\n",
      "After 4700 steps, perplexity is 267.493\n",
      "After 4800 steps, perplexity is 265.221\n",
      "After 4900 steps, perplexity is 263.514\n",
      "After 5000 steps, perplexity is 262.062\n",
      "After 5100 steps, perplexity is 260.121\n",
      "After 5200 steps, perplexity is 258.102\n",
      "After 5300 steps, perplexity is 256.584\n",
      "After 5400 steps, perplexity is 254.985\n",
      "After 5500 steps, perplexity is 253.599\n",
      "After 5600 steps, perplexity is 252.491\n",
      "After 5700 steps, perplexity is 251.022\n",
      "After 5800 steps, perplexity is 249.676\n",
      "After 5900 steps, perplexity is 248.604\n",
      "After 6000 steps, perplexity is 247.511\n",
      "After 6100 steps, perplexity is 246.004\n",
      "After 6200 steps, perplexity is 244.791\n",
      "After 6300 steps, perplexity is 243.814\n",
      "After 6400 steps, perplexity is 242.438\n",
      "After 6500 steps, perplexity is 241.268\n",
      "After 6600 steps, perplexity is 239.889\n",
      "After 6700 steps, perplexity is 239.019\n",
      "After 6800 steps, perplexity is 237.824\n",
      "After 6900 steps, perplexity is 237.086\n",
      "After 7000 steps, perplexity is 236.073\n",
      "After 7100 steps, perplexity is 235.114\n",
      "After 7200 steps, perplexity is 234.281\n",
      "After 7300 steps, perplexity is 233.610\n",
      "After 7400 steps, perplexity is 232.494\n",
      "After 7500 steps, perplexity is 231.580\n",
      "After 7600 steps, perplexity is 230.857\n",
      "After 7700 steps, perplexity is 230.024\n",
      "After 7800 steps, perplexity is 229.051\n",
      "After 7900 steps, perplexity is 227.944\n",
      "After 8000 steps, perplexity is 227.350\n",
      "After 8100 steps, perplexity is 226.328\n",
      "After 8200 steps, perplexity is 225.796\n",
      "After 8300 steps, perplexity is 225.047\n",
      "After 8400 steps, perplexity is 224.381\n",
      "After 8500 steps, perplexity is 223.793\n",
      "After 8600 steps, perplexity is 223.266\n",
      "After 8700 steps, perplexity is 222.502\n",
      "After 8800 steps, perplexity is 221.685\n",
      "After 8900 steps, perplexity is 221.097\n",
      "After 9000 steps, perplexity is 220.571\n",
      "After 9100 steps, perplexity is 219.772\n",
      "After 9200 steps, perplexity is 218.906\n",
      "After 9300 steps, perplexity is 218.410\n",
      "After 9400 steps, perplexity is 217.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 9500 steps, perplexity is 217.173\n",
      "After 9600 steps, perplexity is 216.635\n",
      "After 9700 steps, perplexity is 216.053\n",
      "After 9800 steps, perplexity is 215.561\n",
      "After 9900 steps, perplexity is 215.138\n",
      "After 10000 steps, perplexity is 214.627\n",
      "After 10100 steps, perplexity is 213.957\n",
      "After 10200 steps, perplexity is 213.425\n",
      "After 10300 steps, perplexity is 213.026\n",
      "After 10400 steps, perplexity is 212.361\n",
      "After 10500 steps, perplexity is 211.709\n",
      "After 10600 steps, perplexity is 211.183\n",
      "After 10700 steps, perplexity is 210.690\n",
      "After 10800 steps, perplexity is 210.212\n",
      "After 10900 steps, perplexity is 209.884\n",
      "After 11000 steps, perplexity is 209.345\n",
      "After 11100 steps, perplexity is 208.874\n",
      "After 11200 steps, perplexity is 208.500\n",
      "After 11300 steps, perplexity is 208.171\n",
      "After 11400 steps, perplexity is 207.605\n",
      "After 11500 steps, perplexity is 207.158\n",
      "After 11600 steps, perplexity is 206.878\n",
      "After 11700 steps, perplexity is 206.360\n",
      "After 11800 steps, perplexity is 205.904\n",
      "After 11900 steps, perplexity is 205.305\n",
      "After 12000 steps, perplexity is 205.033\n",
      "After 12100 steps, perplexity is 204.516\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader\n",
    "\n",
    "# 存放数据的路径\n",
    "DATA_PATH = \"PTB dataset\"\n",
    "hidden_size = 200    # 隐藏层，用于记忆和储存过去状态的节点个数\n",
    "num_layers = 2  # LSTM结构的层数为2层，前一层的LSTM的输出作为后一层的输入\n",
    "vocab_size = 10000  # 词典大小，可以存储10000个\n",
    "\n",
    "learning_rate = 1.0  # 初始学习率\n",
    "train_batch_size = 20  # 训练batch大小\n",
    "train_num_step = 35  # 一个训练序列长度\n",
    "num_epoch = 2\n",
    "keep_prob = 0.5  # 节点保存50%\n",
    "max_grad_norm = 5  # 用于控制梯度膨胀（误差对输入层的偏导趋于无穷大）\n",
    "\n",
    "# 在测试时不用限制序列长度\n",
    "eval_batch_size = 1\n",
    "eval_num_step = 1\n",
    "\n",
    "\n",
    "class PTBModel(object):   # 类要使用camelcase格式\n",
    "    def __init__(self, is_training, batch_size, num_steps):  # 初始化属性\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # 定义输入层，输入层维度为batch_size * num_steps\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        # 定义正确输出\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        # 定义lstm结构\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "        if is_training:\n",
    "            # 使用dropout\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)    # 实现多层LSTM\n",
    "\n",
    "        # 将lstm中的状态初始化为全0数组，BasicLSTMCell提供了zero_state来生成全0数组\n",
    "        # batch_size给出了一个batch的大小\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # 生成单词向量，单词总数为10000，单词向量维度为hidden_size200，所以词嵌入总数embedding为\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])\n",
    "\n",
    "        # lstm输入单词为batch_size*num_steps个单词，则输入维度为batch_size*num_steps*hidden_size\n",
    "        # embedding_lookup为将input_data作为索引来搜索embedding中内容，若input_data为[0,0],则输出为embedding中第0个词向量\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # 在训练时用dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        # 输出层\n",
    "        outputs = []\n",
    "        # state为不同batch中的LSTM状态，初始状态为0\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    # variables复用\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                # 将当前输入进lstm中,inputs输入维度为batch_size*num_steps*hidden_size\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                # 输出队列\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        # 输出队列为[batch, hidden_size*num_steps]，在改成[batch*num_steps, hidden_size]\n",
    "        # [-1, hidden_size]中-1表示任意数量的样本\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_size])\n",
    "\n",
    "        # lstm的输出经过全连接层得到最后结果，最后结果的维度是10000，softmax后表明下一个单词的位置（概率大小）\n",
    "        weight = tf.get_variable(\"weight\", [hidden_size, vocab_size])\n",
    "        bias = tf.get_variable(\"bias\", [vocab_size])\n",
    "        logits = tf.matmul(output, weight) + bias  # 预测的结果\n",
    "\n",
    "        # 交叉熵损失，tensorflow中有sequence_loss_by_example来计算一个序列的交叉熵损失和\n",
    "        # tf.reshape将正确结果转换为一维的,tf.ones建立损失权重，所有权重都为1，不同时刻不同batch的权重是一样的\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self.targets, [-1])],\n",
    "                                                      [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "\n",
    "        # 每个batch的平均损失,reduce_sum计算loss总和\n",
    "        self.cost = tf.reduce_sum(loss)/batch_size\n",
    "        self.final_state = state\n",
    "\n",
    "        # 在训练时定义反向传播\n",
    "        if not is_training:\n",
    "            return\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # 使用clip_by_global_norm控制梯度大小，避免梯度膨胀\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), max_grad_norm)\n",
    "        # 梯度下降优化\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # 训练步骤,apply_gradients将计算出的梯度应用到变量上\n",
    "        # zip将grads和trainable_variables中每一个打包成元组\n",
    "        # a = [1,2,3]， b = [4,5,6]， zip(a, b)： [(1, 4), (2, 5), (3, 6)]\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "\n",
    "# 模型训练，给出模型的复杂度\n",
    "def run_epoch(session, model, data, train_op, output_log, epoch_size):\n",
    "    # perplexity（复杂度）是用来评价一个语言模型预测一个样本是否很好的标准。复杂度越低，代表模型的预测性能越好\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    # 训练一个epoch\n",
    "    for step in range(epoch_size):\n",
    "        x, y = session.run(data)\n",
    "        # cost是交叉熵损失，即下一个单词为给定单词的概率\n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op],\n",
    "                                     {model.input_data: x, model.targets: y, model.initial_state: state})\n",
    "        # 将所有batch、时刻的损失相加\n",
    "        total_costs += cost\n",
    "        # 所有epoch总输出单词数\n",
    "        iters += model.num_steps\n",
    "\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (step, np.exp(total_costs / iters)))\n",
    "\n",
    "    # 返回语言模型的perplexity值\n",
    "    return np.exp(total_costs / iters)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 获取数据\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)\n",
    "\n",
    "    # 计算一个epoch需要训练的次数\n",
    "    train_data_len = len(train_data)\n",
    "    train_epoch_size = (train_data_len - 1)\n",
    "\n",
    "    valid_data_len = len(valid_data)\n",
    "    valid_epoch_size = (valid_data_len - 1)\n",
    "\n",
    "    test_data_len = len(test_data)\n",
    "    test_epoch_size = (test_data_len - 1)\n",
    "\n",
    "    # 定义初始化函数\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "\n",
    "    # 定义语言训练模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, train_batch_size, train_num_step)\n",
    "\n",
    "    # 定义语言测试模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, eval_batch_size, eval_num_step)\n",
    "\n",
    "    # 训练模型\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        train_queue = reader.ptb_producer(train_data, train_model.batch_size, train_model.num_steps)\n",
    "        eval_queue = reader.ptb_producer(valid_data, eval_model.batch_size, eval_model.num_steps)\n",
    "        test_queue = reader.ptb_producer(test_data, eval_model.batch_size, eval_model.num_steps)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "\n",
    "        for i in range(num_epoch):\n",
    "            print(\"iteration: %d\" % (i + 1))\n",
    "            run_epoch(session, train_model, train_queue, train_model.train_op, True, train_epoch_size)\n",
    "            # 传入了tf.no_op表示不进行优化\n",
    "            valid_perplexity = run_epoch(session, eval_model, eval_queue, tf.no_op(), False, valid_epoch_size)\n",
    "            print(\"Epoch: %d Validation Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity = run_epoch(session, eval_model, test_queue, tf.no_op(), False, test_epoch_size)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
