{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载正负评论数据\n",
    "\n",
    "train_pos_text = []\n",
    "train_neg_text = []\n",
    "validation_pos_text = []\n",
    "validation_neg_text = []\n",
    "test_pos_text = []\n",
    "test_neg_text = []\n",
    "\n",
    "with open(\"Dataset/train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"1\":\n",
    "            train_pos_text.append(line[2:])\n",
    "        else:\n",
    "            train_neg_text.append(line[2:])\n",
    "with open(\"Dataset/validation.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"1\":\n",
    "            validation_pos_text.append(line[2:])\n",
    "        else:\n",
    "            validation_neg_text.append(line[2:]) \n",
    "with open(\"Dataset/test.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"1\":\n",
    "            test_pos_text.append(line[2:])\n",
    "        else:\n",
    "            test_neg_text.append(line[2:]) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- POSITIVE TEXT --------------------\n",
      "Total positive sentences: 9999\n",
      "The average length of positive sentences: 44.47994799479948\n",
      "The max length of positive sentences: 679\n",
      "The min length of positive sentences: 31\n",
      "Most common words in positive sentences: \n",
      "[('我', 9352), ('看', 5728), ('不', 4883), ('也', 4261), ('电影', 4196), ('你', 3205), ('很', 3030), ('人', 2979), ('说', 2475), ('没有', 2425), ('一个', 2368), ('好', 2196), ('让', 2160), ('就是', 2145), ('没', 2070), ('给', 2024), ('还是', 2023), ('什么', 1883), ('剧情', 1717), ('片子', 1636), ('想', 1628), ('去', 1575), ('太', 1540), ('觉得', 1538), ('不是', 1538), ('他', 1499), ('被', 1442), ('故事', 1440), ('这个', 1434), ('要', 1420), ('能', 1398), ('但', 1388), ('个', 1378), ('导演', 1369), ('又', 1362), ('这部', 1336), ('把', 1244), ('对', 1240), ('一部', 1238), ('拍', 1233), ('这么', 1225), ('上', 1223), ('最后', 1196), ('可以', 1194), ('多', 1161), ('那么', 1114), ('但是', 1109), ('自己', 1107), ('喜欢', 1074), ('完全', 1062), ('知道', 1056), ('怎么', 1055), ('完', 1052), ('感觉', 1014), ('真的', 985), ('还有', 944), ('这样', 903), ('实在', 902), ('来', 901), ('看到', 859), ('她', 848), ('跟', 843), ('会', 837), ('再', 834), ('演员', 831), ('时候', 830), ('真是', 819), ('那个', 815), ('里', 782), ('大', 767), ('情节', 748), ('像', 739), ('这种', 728), ('中', 723), ('一', 722), ('烂片', 721), ('最', 716), ('如果', 703), ('不能', 694), ('而', 693), ('片', 681), ('真', 662), ('演技', 653), ('不过', 649), ('编剧', 646), ('才', 646), ('为了', 630), ('小', 626), ('烂', 624), ('出来', 622), ('死', 619), ('其实', 616), ('一点', 614), ('爱情', 610), ('更', 601), ('因为', 598), ('不错', 587), ('爱', 585), ('它', 575), ('比', 573)]\n",
      "\n",
      "-------------------- NEGATIVE TEXT --------------------\n",
      "Total negative sentences: 9999\n",
      "The average length of negative sentences: 44.878087808780876\n",
      "The max length of negative sentences: 119\n",
      "The min length of negative sentences: 31\n",
      "Most common words in negative sentences: \n",
      "[('我', 9571), ('很', 5033), ('看', 4672), ('人', 4517), ('也', 4285), ('电影', 4132), ('不', 3648), ('你', 3430), ('一个', 2845), ('让', 2731), ('他', 2505), ('好', 2429), ('还是', 2137), ('喜欢', 2105), ('没有', 2059), ('自己', 2034), ('说', 1948), ('但', 1918), ('会', 1751), ('就是', 1716), ('我们', 1710), ('觉得', 1667), ('对', 1623), ('又', 1548), ('但是', 1528), ('最后', 1527), ('这部', 1450), ('爱', 1438), ('被', 1427), ('中', 1375), ('时候', 1354), ('去', 1330), ('这个', 1327), ('故事', 1325), ('不是', 1320), ('片子', 1289), ('能', 1268), ('给', 1241), ('而', 1226), ('上', 1224), ('想', 1222), ('要', 1191), ('可以', 1190), ('什么', 1177), ('里', 1176), ('却', 1169), ('最', 1168), ('她', 1167), ('真的', 1162), ('一部', 1156), ('与', 1145), ('多', 1071), ('感觉', 1063), ('知道', 1052), ('那么', 1044), ('这样', 998), ('爱情', 993), ('看到', 987), ('虽然', 985), ('太', 983), ('把', 965), ('生活', 947), ('没', 906), ('那个', 889), ('因为', 882), ('不过', 880), ('完', 874), ('其实', 872), ('只是', 861), ('剧情', 859), ('不错', 855), ('才', 854), ('更', 834), ('个', 832), ('很多', 811), ('他们', 787), ('像', 779), ('还有', 762), ('它', 762), ('小', 759), ('有点', 755), ('开始', 745), ('影片', 743), ('从', 734), ('如果', 733), ('结局', 726), ('一直', 709), ('一样', 688), ('世界', 682), ('过', 681), ('感动', 658), ('再', 657), ('一起', 652), ('做', 652), ('这么', 620), ('来', 615), ('时', 607), ('导演', 598), ('那些', 573), ('这种', 558)]\n"
     ]
    }
   ],
   "source": [
    "# 积极文本统计\n",
    "print(\"-\" * 20 + \" POSITIVE TEXT \" + \"-\" * 20)\n",
    "# 分句\n",
    "print(\"Total positive sentences: {}\".format(len(train_pos_text)))\n",
    "print(\"The average length of positive sentences: {}\".format(np.mean([len(sentence.split()) for sentence in train_pos_text])))\n",
    "print(\"The max length of positive sentences: {}\".format(np.max([len(sentence.split()) for sentence in train_pos_text])))\n",
    "print(\"The min length of positive sentences: {}\".format(np.min([len(sentence.split()) for sentence in train_pos_text])))\n",
    "# 统计高频词\n",
    "train_pos_word = []\n",
    "for sentense in train_pos_text:\n",
    "    word_list = sentense.split()\n",
    "    train_pos_word.append(word_list)\n",
    "train_pos_w = sum(train_pos_word, [])\n",
    "\n",
    "validation_pos_word = []\n",
    "for sentense in validation_pos_text:\n",
    "    word_list = sentense.split()\n",
    "    validation_pos_word.append(word_list)\n",
    "validation_pos_w = sum(validation_pos_word, [])\n",
    "\n",
    "test_pos_word = []\n",
    "for sentense in test_pos_text:\n",
    "    word_list = sentense.split()\n",
    "    test_pos_word.append(word_list)\n",
    "test_pos_w = sum(test_pos_word, [])\n",
    "\n",
    "# print(pos_word[0])\n",
    "c = Counter(train_pos_w).most_common(100)\n",
    "print(\"Most common words in positive sentences: \\n{}\".format(c))\n",
    "\n",
    "# 校级文本统计\n",
    "print()\n",
    "print(\"-\" * 20 + \" NEGATIVE TEXT \" + \"-\" * 20)\n",
    "# 分句\n",
    "print(\"Total negative sentences: {}\".format(len(train_neg_text)))\n",
    "print(\"The average length of negative sentences: {}\".format(np.mean([len(sentence.split()) for sentence in train_neg_text])))\n",
    "print(\"The max length of negative sentences: {}\".format(np.max([len(sentence.split()) for sentence in train_neg_text])))\n",
    "print(\"The min length of negative sentences: {}\".format(np.min([len(sentence.split()) for sentence in train_neg_text])))\n",
    "# 统计高频词\n",
    "train_neg_word = []\n",
    "for sentense in train_neg_text:\n",
    "    word_list = sentense.split()\n",
    "    train_neg_word.append(word_list)\n",
    "train_neg_w = sum(train_neg_word, [])\n",
    "\n",
    "validation_neg_word = []\n",
    "for sentense in validation_neg_text:\n",
    "    word_list = sentense.split()\n",
    "    validation_neg_word.append(word_list)\n",
    "validation_neg_w = sum(validation_neg_word, [])\n",
    "\n",
    "test_neg_word = []\n",
    "for sentense in test_neg_text:\n",
    "    word_list = sentense.split()\n",
    "    test_neg_word.append(word_list)\n",
    "test_neg_w = sum(test_neg_word, [])\n",
    "c = Counter(train_neg_w).most_common(100)\n",
    "print(\"Most common words in negative sentences: \\n{}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('安圣基', 1),\n",
       " ('坦', 1),\n",
       " ('第几个', 1),\n",
       " ('可肿', 1),\n",
       " ('情妇', 1),\n",
       " ('不净', 1),\n",
       " ('伊斯特', 1),\n",
       " ('姑奶奶', 1),\n",
       " ('狀況', 1),\n",
       " ('染色体', 1),\n",
       " ('松软', 1),\n",
       " ('周期性', 1),\n",
       " ('经脉', 1),\n",
       " ('满点', 1),\n",
       " ('加两星', 1),\n",
       " ('登峰', 1),\n",
       " ('趾高气昂', 1),\n",
       " ('有益于', 1),\n",
       " ('穿着打扮', 1),\n",
       " ('红土', 1),\n",
       " ('中學', 1),\n",
       " ('遗址', 1),\n",
       " ('仓央', 1),\n",
       " ('家书', 1),\n",
       " ('烦死人', 1),\n",
       " ('徒留', 1),\n",
       " ('赶回来', 1),\n",
       " ('尔后', 1),\n",
       " ('眠', 1),\n",
       " ('这带', 1),\n",
       " ('拼出', 1),\n",
       " ('数倍', 1),\n",
       " ('强奸犯', 1),\n",
       " ('格雷格', 1),\n",
       " ('巨大成功', 1),\n",
       " ('马麻', 1),\n",
       " ('愿用', 1),\n",
       " ('那组', 1),\n",
       " ('傑出', 1),\n",
       " ('水声', 1),\n",
       " ('品不出', 1),\n",
       " ('三味', 1),\n",
       " ('男生女生', 1),\n",
       " ('赵武', 1),\n",
       " ('逝', 1),\n",
       " ('头部', 1),\n",
       " ('人間', 1),\n",
       " ('金说', 1),\n",
       " ('达时', 1),\n",
       " ('时内', 1),\n",
       " ('化成灰', 1),\n",
       " ('那袭', 1),\n",
       " ('地儿', 1),\n",
       " ('困窘', 1),\n",
       " ('几有', 1),\n",
       " ('纵有', 1),\n",
       " ('御医', 1),\n",
       " ('betray', 1),\n",
       " ('大众文化', 1),\n",
       " ('保持沉默', 1),\n",
       " ('我长', 1),\n",
       " ('数据分析', 1),\n",
       " ('烂造', 1),\n",
       " ('這集', 1),\n",
       " ('白金汉', 1),\n",
       " ('Detective', 1),\n",
       " ('倾向性', 1),\n",
       " ('雖有', 1),\n",
       " ('血红色', 1),\n",
       " ('祭拜', 1),\n",
       " ('世袭', 1),\n",
       " ('宫泽里惠', 1),\n",
       " ('恍悟', 1),\n",
       " ('宋先生', 1),\n",
       " ('见过世面', 1),\n",
       " ('經常', 1),\n",
       " ('三思', 1),\n",
       " ('难养', 1),\n",
       " ('ａ', 1),\n",
       " ('凌乱不堪', 1),\n",
       " ('方可', 1),\n",
       " ('坏脾气', 1),\n",
       " ('剥光', 1),\n",
       " ('毕福剑', 1),\n",
       " ('Vaughn', 1),\n",
       " ('腐尔', 1),\n",
       " ('血流', 1),\n",
       " ('倔', 1),\n",
       " ('此世', 1),\n",
       " ('理不糙', 1),\n",
       " ('熬不住', 1),\n",
       " ('sometime', 1),\n",
       " ('黄韵玲', 1),\n",
       " ('戚', 1),\n",
       " ('大梦', 1),\n",
       " ('刷屏', 1),\n",
       " ('腐臭', 1),\n",
       " ('副产品', 1),\n",
       " ('强哥', 1),\n",
       " ('一炉', 1),\n",
       " ('拳打', 1),\n",
       " ('晴空', 1),\n",
       " ('說還', 1),\n",
       " ('文惠', 1),\n",
       " ('煮熟', 1),\n",
       " ('目不忍视', 1),\n",
       " ('秦孝公', 1),\n",
       " ('极限运动', 1),\n",
       " ('破坏者', 1),\n",
       " ('成诗', 1),\n",
       " ('重于', 1),\n",
       " ('啊親', 1),\n",
       " ('到些', 1),\n",
       " ('车型', 1),\n",
       " ('竹内', 1),\n",
       " ('那三人', 1),\n",
       " ('桅杆', 1),\n",
       " ('专', 1),\n",
       " ('flipped', 1),\n",
       " ('拍過', 1),\n",
       " ('蜂窝', 1),\n",
       " ('del', 1),\n",
       " ('未见得', 1),\n",
       " ('哄人', 1),\n",
       " ('Steel', 1),\n",
       " ('不得善终', 1),\n",
       " ('坐怀不乱', 1),\n",
       " ('還是給', 1),\n",
       " ('达赖喇嘛', 1),\n",
       " ('简化', 1),\n",
       " ('两首', 1),\n",
       " ('归纳', 1),\n",
       " ('人生旅途', 1),\n",
       " ('毫不相关', 1),\n",
       " ('小感', 1),\n",
       " ('神器', 1),\n",
       " ('穿针引线', 1),\n",
       " ('被雷', 1),\n",
       " ('Blu', 1),\n",
       " ('中算', 1),\n",
       " ('我校', 1),\n",
       " ('迎美', 1),\n",
       " ('愚见', 1),\n",
       " ('拿破仑', 1),\n",
       " ('Zodiac', 1),\n",
       " ('立体声', 1),\n",
       " ('足本', 1),\n",
       " ('spark', 1),\n",
       " ('勾走', 1),\n",
       " ('拉斯维加斯', 1),\n",
       " ('中校', 1),\n",
       " ('马利克', 1),\n",
       " ('神呐', 1),\n",
       " ('AL', 1),\n",
       " ('寡佬', 1),\n",
       " ('甜品店', 1),\n",
       " ('推行', 1),\n",
       " ('慢悠', 1),\n",
       " ('布朗宁', 1),\n",
       " ('对唱', 1),\n",
       " ('召回', 1),\n",
       " ('灾区', 1),\n",
       " ('个人经历', 1),\n",
       " ('邢台', 1),\n",
       " ('留守', 1),\n",
       " ('片们', 1),\n",
       " ('欲火焚身', 1),\n",
       " ('焕发', 1),\n",
       " ('酒水', 1),\n",
       " ('魁北克', 1),\n",
       " ('决定权', 1),\n",
       " ('负面影响', 1),\n",
       " ('现已', 1),\n",
       " ('周嘉玲', 1),\n",
       " ('呆湾', 1),\n",
       " ('Parker', 1),\n",
       " ('百分之八十', 1),\n",
       " ('令人担心', 1),\n",
       " ('赌圣', 1),\n",
       " ('今典', 1),\n",
       " ('冬夏', 1),\n",
       " ('抵押', 1),\n",
       " ('交了', 1),\n",
       " ('百感', 1),\n",
       " ('第一处', 1),\n",
       " ('爱米莉', 1),\n",
       " ('孤影', 1),\n",
       " ('佳能', 1),\n",
       " ('诸葛', 1),\n",
       " ('选方启宏', 1),\n",
       " ('说别', 1),\n",
       " ('三眼', 1),\n",
       " ('提要', 1),\n",
       " ('胳肢窝', 1),\n",
       " ('晏好', 1),\n",
       " ('渐变', 1),\n",
       " ('咱家', 1),\n",
       " ('潘神', 1),\n",
       " ('麋鹿', 1),\n",
       " ('该补', 1),\n",
       " ('自习课', 1),\n",
       " ('赶回', 1),\n",
       " ('这笔', 1),\n",
       " ('肉片', 1),\n",
       " ('严谨性', 1),\n",
       " ('物资', 1),\n",
       " ('machine', 1),\n",
       " ('仅此', 1),\n",
       " ('惠子', 1),\n",
       " ('生父', 1),\n",
       " ('穿墙', 1),\n",
       " ('电子书', 1),\n",
       " ('共和国', 1),\n",
       " ('蛮虐', 1),\n",
       " ('恒', 1),\n",
       " ('鞭子', 1),\n",
       " ('乱弹', 1),\n",
       " ('亚克', 1),\n",
       " ('母牛', 1),\n",
       " ('耗掉', 1),\n",
       " ('徐熙媛', 1),\n",
       " ('還算', 1),\n",
       " ('一紧', 1),\n",
       " ('Despicable', 1),\n",
       " ('恋曲', 1),\n",
       " ('calm', 1),\n",
       " ('友朋', 1),\n",
       " ('蠢材', 1),\n",
       " ('千层浪', 1),\n",
       " ('高效率', 1),\n",
       " ('合不拢嘴', 1),\n",
       " ('钱钟书', 1),\n",
       " ('很象', 1),\n",
       " ('did', 1),\n",
       " ('逃不脱', 1),\n",
       " ('宁谧', 1),\n",
       " ('甲板', 1),\n",
       " ('比穆赫兰道', 1),\n",
       " ('犯了错', 1),\n",
       " ('谈心', 1),\n",
       " ('能学', 1),\n",
       " ('aoi', 1),\n",
       " ('港人', 1),\n",
       " ('喷气', 1),\n",
       " ('鞭尸', 1),\n",
       " ('故居', 1),\n",
       " ('空手而归', 1),\n",
       " ('过过瘾', 1),\n",
       " ('吐得', 1),\n",
       " ('插得', 1),\n",
       " ('牛比', 1),\n",
       " ('愛美麗', 1),\n",
       " ('没掉', 1),\n",
       " ('毛非', 1),\n",
       " ('诸脑', 1),\n",
       " ('吸管', 1),\n",
       " ('展望未来', 1),\n",
       " ('公仆', 1),\n",
       " ('雪梨', 1),\n",
       " ('聊得', 1),\n",
       " ('实名制', 1),\n",
       " ('补贴', 1),\n",
       " ('乱说', 1),\n",
       " ('沉于', 1),\n",
       " ('修仙', 1),\n",
       " ('扎进', 1),\n",
       " ('卧倒', 1),\n",
       " ('野蠻', 1),\n",
       " ('南柯一梦', 1),\n",
       " ('lucas', 1),\n",
       " ('PW', 1),\n",
       " ('伤怀', 1),\n",
       " ('手链', 1),\n",
       " ('事业成功', 1),\n",
       " ('碾', 1),\n",
       " ('左撇子', 1),\n",
       " ('咕咕', 1),\n",
       " ('CC', 1),\n",
       " ('根锡', 1),\n",
       " ('打高', 1),\n",
       " ('项伯', 1),\n",
       " ('点化', 1),\n",
       " ('冲王', 1),\n",
       " ('老伍', 1),\n",
       " ('光会', 1),\n",
       " ('如命', 1),\n",
       " ('提亮', 1),\n",
       " ('领证', 1),\n",
       " ('blablabla', 1),\n",
       " ('好心人', 1),\n",
       " ('惊叹不已', 1),\n",
       " ('一帝', 1),\n",
       " ('靠边', 1),\n",
       " ('É', 1),\n",
       " ('磨擦', 1),\n",
       " ('伴有', 1),\n",
       " ('三杯', 1),\n",
       " ('发展缓慢', 1),\n",
       " ('半头', 1),\n",
       " ('由纪夫', 1),\n",
       " ('小宝宝', 1),\n",
       " ('德里', 1),\n",
       " ('难过地', 1),\n",
       " ('萌着', 1),\n",
       " ('ATV', 1),\n",
       " ('丫们', 1),\n",
       " ('仅止', 1),\n",
       " ('老文青', 1),\n",
       " ('跺脚', 1),\n",
       " ('雞皮', 1),\n",
       " ('本田', 1),\n",
       " ('山口智子', 1),\n",
       " ('字句', 1),\n",
       " ('推入', 1),\n",
       " ('唐国强', 1),\n",
       " ('很特別', 1),\n",
       " ('成事不足', 1),\n",
       " ('咽下', 1),\n",
       " ('估算', 1),\n",
       " ('那海', 1),\n",
       " ('转校', 1),\n",
       " ('Sad', 1),\n",
       " ('HH', 1),\n",
       " ('多天', 1),\n",
       " ('sooooo', 1),\n",
       " ('售票处', 1),\n",
       " ('就近', 1),\n",
       " ('美服', 1),\n",
       " ('最後結局', 1),\n",
       " ('微波', 1),\n",
       " ('蒂诺', 1),\n",
       " ('摘要', 1),\n",
       " ('定数', 1),\n",
       " ('建个', 1),\n",
       " ('穷死', 1),\n",
       " ('Dakota', 1),\n",
       " ('李民基', 1),\n",
       " ('不艳', 1),\n",
       " ('雌雄同体', 1),\n",
       " ('武痴', 1),\n",
       " ('一拉', 1),\n",
       " ('重型', 1),\n",
       " ('周元', 1),\n",
       " ('额叶', 1),\n",
       " ('炸药包', 1),\n",
       " ('挺重', 1),\n",
       " ('键入', 1),\n",
       " ('戏剧学院', 1),\n",
       " ('耽', 1),\n",
       " ('弘树', 1),\n",
       " ('递', 1),\n",
       " ('加好', 1),\n",
       " ('一面倒', 1),\n",
       " ('盼望着', 1),\n",
       " ('晨曦', 1),\n",
       " ('钻出来', 1),\n",
       " ('大雷片', 1),\n",
       " ('專一', 1),\n",
       " ('主宰者', 1),\n",
       " ('被忽视', 1),\n",
       " ('最低谷', 1),\n",
       " ('一夜间', 1),\n",
       " ('GOGO', 1),\n",
       " ('第一间', 1),\n",
       " ('霍建華', 1),\n",
       " ('接听', 1),\n",
       " ('大虐', 1),\n",
       " ('全露', 1),\n",
       " ('代表作品', 1),\n",
       " ('烟云', 1),\n",
       " ('空军', 1),\n",
       " ('美煞', 1),\n",
       " ('赶到', 1),\n",
       " ('小火', 1),\n",
       " ('心走', 1),\n",
       " ('白晶晶', 1),\n",
       " ('飞行服', 1),\n",
       " ('创业史', 1),\n",
       " ('此本', 1),\n",
       " ('异乡人', 1),\n",
       " ('会泪', 1),\n",
       " ('Its', 1),\n",
       " ('湘伦', 1),\n",
       " ('廣東話', 1),\n",
       " ('此部', 1),\n",
       " ('不足为外人道', 1),\n",
       " ('记挂着', 1),\n",
       " ('explain', 1),\n",
       " ('供应商', 1),\n",
       " ('感嘛', 1),\n",
       " ('小号手', 1),\n",
       " ('永远快乐', 1),\n",
       " ('山脚', 1),\n",
       " ('语塞', 1),\n",
       " ('冻结', 1),\n",
       " ('遇不上', 1),\n",
       " ('小步舞曲', 1),\n",
       " ('猪脑', 1),\n",
       " ('清涩', 1),\n",
       " ('湊合', 1),\n",
       " ('假面具', 1),\n",
       " ('渗入', 1),\n",
       " ('多一點', 1),\n",
       " ('正餐', 1),\n",
       " ('列車', 1),\n",
       " ('粗放', 1),\n",
       " ('吸着', 1),\n",
       " ('显不出', 1),\n",
       " ('严严实实', 1),\n",
       " ('健美', 1),\n",
       " ('这丫', 1),\n",
       " ('映画', 1),\n",
       " ('往回', 1),\n",
       " ('有志气', 1),\n",
       " ('擦洗', 1),\n",
       " ('遇刺', 1),\n",
       " ('打的好', 1),\n",
       " ('伤城', 1),\n",
       " ('Butterfly', 1),\n",
       " ('于理', 1),\n",
       " ('科学幻想', 1),\n",
       " ('中连', 1),\n",
       " ('干系', 1),\n",
       " ('花点', 1),\n",
       " ('三路', 1),\n",
       " ('人赞', 1),\n",
       " ('徐正曦', 1),\n",
       " ('星际大战', 1),\n",
       " ('而终', 1),\n",
       " ('charles', 1),\n",
       " ('交由', 1),\n",
       " ('不知去向', 1),\n",
       " ('下成', 1),\n",
       " ('七七八八', 1),\n",
       " ('激流', 1),\n",
       " ('割断', 1),\n",
       " ('这宁', 1),\n",
       " ('顾全大局', 1),\n",
       " ('曹公', 1),\n",
       " ('高看', 1),\n",
       " ('北朝鲜', 1),\n",
       " ('攻略', 1),\n",
       " ('节后', 1),\n",
       " ('长吁短叹', 1),\n",
       " ('野心家', 1),\n",
       " ('大忠', 1),\n",
       " ('步枪', 1),\n",
       " ('秋生哥', 1),\n",
       " ('称其为', 1),\n",
       " ('操劳', 1),\n",
       " ('地窖', 1),\n",
       " ('申奥片', 1),\n",
       " ('有两下子', 1),\n",
       " ('FAST', 1),\n",
       " ('唐娜', 1),\n",
       " ('缓捐', 1),\n",
       " ('思考性', 1),\n",
       " ('夺回', 1),\n",
       " ('明眸', 1),\n",
       " ('薄情郎', 1),\n",
       " ('女上司', 1),\n",
       " ('阿九', 1),\n",
       " ('大耳', 1),\n",
       " ('七仙女', 1),\n",
       " ('这样一来', 1),\n",
       " ('中外合资', 1),\n",
       " ('没选好', 1),\n",
       " ('英武', 1),\n",
       " ('公母', 1),\n",
       " ('昂辉', 1),\n",
       " ('周伯通', 1),\n",
       " ('男去', 1),\n",
       " ('大半生', 1),\n",
       " ('井', 1),\n",
       " ('fairy', 1),\n",
       " ('海角天涯', 1),\n",
       " ('moral', 1),\n",
       " ('套住', 1),\n",
       " ('短路', 1),\n",
       " ('涛哥', 1),\n",
       " ('行情', 1),\n",
       " ('表演系', 1),\n",
       " ('沁人心脾', 1),\n",
       " ('比较严重', 1),\n",
       " ('国际友人', 1),\n",
       " ('GOOD', 1),\n",
       " ('那顶', 1),\n",
       " ('婚姻关系', 1),\n",
       " ('很胖', 1),\n",
       " ('迸射', 1),\n",
       " ('硬硬', 1),\n",
       " ('造个', 1),\n",
       " ('clooney', 1),\n",
       " ('死不足惜', 1),\n",
       " ('コ', 1),\n",
       " ('有多苦', 1),\n",
       " ('增高', 1),\n",
       " ('下个世纪', 1),\n",
       " ('累心', 1),\n",
       " ('阴损', 1),\n",
       " ('熟睡', 1),\n",
       " ('片商', 1),\n",
       " ('熟识', 1),\n",
       " ('别坑', 1),\n",
       " ('泥塑', 1),\n",
       " ('有钱有势', 1),\n",
       " ('谨小慎微', 1),\n",
       " ('对静秋', 1),\n",
       " ('接通', 1),\n",
       " ('mammy', 1),\n",
       " ('斯大', 1),\n",
       " ('买来', 1),\n",
       " ('自以', 1),\n",
       " ('性饥渴', 1),\n",
       " ('要评', 1),\n",
       " ('鸟蛋', 1),\n",
       " ('祥仔', 1),\n",
       " ('转轮', 1),\n",
       " ('輕松', 1),\n",
       " ('yesterday', 1),\n",
       " ('万米', 1),\n",
       " ('fucked', 1),\n",
       " ('横死', 1),\n",
       " ('看用', 1),\n",
       " ('物质文明', 1),\n",
       " ('疼疼', 1),\n",
       " ('上吊自杀', 1),\n",
       " ('左岸', 1),\n",
       " ('女艺人', 1),\n",
       " ('纯过', 1),\n",
       " ('罗进', 1),\n",
       " ('义肢', 1),\n",
       " ('还存', 1),\n",
       " ('易坏', 1),\n",
       " ('fool', 1),\n",
       " ('指教', 1),\n",
       " ('右脚', 1),\n",
       " ('贺卡', 1),\n",
       " ('事实真相', 1),\n",
       " ('第一二', 1),\n",
       " ('官网', 1),\n",
       " ('常春藤', 1),\n",
       " ('制作组', 1),\n",
       " ('穿山甲', 1),\n",
       " ('样本', 1),\n",
       " ('掺合', 1),\n",
       " ('统治者', 1),\n",
       " ('Park', 1),\n",
       " ('绣花', 1),\n",
       " ('坡', 1),\n",
       " ('tree', 1),\n",
       " ('画境', 1),\n",
       " ('handle', 1),\n",
       " ('力奇', 1),\n",
       " ('释放出来', 1),\n",
       " ('偷跑', 1),\n",
       " ('飞后', 1),\n",
       " ('幾分鐘', 1),\n",
       " ('虞啸卿', 1),\n",
       " ('反手', 1),\n",
       " ('努力做到', 1),\n",
       " ('费用', 1),\n",
       " ('生肉', 1),\n",
       " ('王祖贤版', 1),\n",
       " ('陆明君', 1),\n",
       " ('听出', 1),\n",
       " ('不償命', 1),\n",
       " ('发过', 1),\n",
       " ('路易十六', 1),\n",
       " ('场场', 1),\n",
       " ('法官', 1),\n",
       " ('数遍', 1),\n",
       " ('指指点点', 1),\n",
       " ('眼部', 1),\n",
       " ('禁止', 1),\n",
       " ('邬桑', 1),\n",
       " ('无迹可寻', 1),\n",
       " ('飘离', 1),\n",
       " ('伞兵', 1),\n",
       " ('放声', 1),\n",
       " ('闲下来', 1),\n",
       " ('站街女', 1),\n",
       " ('耀武扬威', 1),\n",
       " ('美嘉在', 1),\n",
       " ('条路', 1),\n",
       " ('望尘莫及', 1),\n",
       " ('遗世', 1),\n",
       " ('峰顶', 1),\n",
       " ('神神经经', 1),\n",
       " ('中小', 1),\n",
       " ('聂远', 1),\n",
       " ('trauma', 1),\n",
       " ('交易员', 1),\n",
       " ('守得云', 1),\n",
       " ('director', 1),\n",
       " ('擅于', 1),\n",
       " ('彻悟', 1),\n",
       " ('傷痛', 1),\n",
       " ('瓶颈', 1),\n",
       " ('TYPE', 1),\n",
       " ('chanel', 1),\n",
       " ('chaos', 1),\n",
       " ('希和子', 1),\n",
       " ('Vampires', 1),\n",
       " ('客户端', 1),\n",
       " ('拉锯', 1),\n",
       " ('润', 1),\n",
       " ('拼着', 1),\n",
       " ('暹羅', 1),\n",
       " ('雨巷', 1),\n",
       " ('联邦', 1),\n",
       " ('离群索居', 1),\n",
       " ('搭救', 1),\n",
       " ('演练', 1),\n",
       " ('宣', 1),\n",
       " ('超慢', 1),\n",
       " ('忍者神龟', 1),\n",
       " ('古哥', 1),\n",
       " ('为题', 1),\n",
       " ('饒', 1),\n",
       " ('掩护', 1),\n",
       " ('figure', 1),\n",
       " ('博文', 1),\n",
       " ('配套', 1),\n",
       " ('死结', 1),\n",
       " ('用功', 1),\n",
       " ('无底', 1),\n",
       " ('grey', 1),\n",
       " ('肿瘤', 1),\n",
       " ('千万分之一', 1),\n",
       " ('破镜', 1),\n",
       " ('菲佛', 1),\n",
       " ('出气', 1),\n",
       " ('过滤', 1),\n",
       " ('发发', 1),\n",
       " ('帅不帅', 1),\n",
       " ('一神', 1),\n",
       " ('和翔', 1),\n",
       " ('濃重', 1),\n",
       " ('没回', 1),\n",
       " ('思想感情', 1),\n",
       " ('挺神', 1),\n",
       " ('赵高', 1),\n",
       " ('松田', 1),\n",
       " ('疫情', 1),\n",
       " ('MBD', 1),\n",
       " ('不速', 1),\n",
       " ('瓜裂', 1),\n",
       " ('人全', 1),\n",
       " ('宝贵时间', 1),\n",
       " ('設置', 1),\n",
       " ('露着', 1),\n",
       " ('钱谦益', 1),\n",
       " ('天妒', 1),\n",
       " ('局势', 1),\n",
       " ('自不量力', 1),\n",
       " ('标清', 1),\n",
       " ('可悲可叹', 1),\n",
       " ('agree', 1),\n",
       " ('孩提时代', 1),\n",
       " ('敏', 1),\n",
       " ('存钱罐', 1),\n",
       " ('附', 1),\n",
       " ('有条不紊', 1),\n",
       " ('无处可去', 1),\n",
       " ('趋炎附势', 1),\n",
       " ('莫再', 1),\n",
       " ('高招', 1),\n",
       " ('不加', 1),\n",
       " ('球技', 1),\n",
       " ('乳白色', 1),\n",
       " ('陈胜吴广', 1),\n",
       " ('周是', 1),\n",
       " ('觀察', 1),\n",
       " ('奇爱', 1),\n",
       " ('延迟', 1),\n",
       " ('天文', 1),\n",
       " ('無以', 1),\n",
       " ('劇情片', 1),\n",
       " ('伸缩', 1),\n",
       " ('看厌', 1),\n",
       " ('魁地奇', 1),\n",
       " ('不育', 1),\n",
       " ('裙下', 1),\n",
       " ('马友友', 1),\n",
       " ('许多部', 1),\n",
       " ('小平头', 1),\n",
       " ('晕厥', 1),\n",
       " ('懷', 1),\n",
       " ('掩耳盗铃', 1),\n",
       " ('尤深', 1),\n",
       " ('二季', 1),\n",
       " ('挑眉', 1),\n",
       " ('署名', 1),\n",
       " ('吕蒙', 1),\n",
       " ('失准', 1),\n",
       " ('我太有', 1),\n",
       " ('科研人员', 1),\n",
       " ('回头草', 1),\n",
       " ('デ', 1),\n",
       " ('煎', 1),\n",
       " ('有播', 1),\n",
       " ('食客', 1),\n",
       " ('twins', 1),\n",
       " ('逗着', 1),\n",
       " ('十八层', 1),\n",
       " ('英國', 1),\n",
       " ('一入', 1),\n",
       " ('艰险', 1),\n",
       " ('走下', 1),\n",
       " ('比鬼', 1),\n",
       " ('干笑', 1),\n",
       " ('差好', 1),\n",
       " ('GUS', 1),\n",
       " ('刻刻', 1),\n",
       " ('求学', 1),\n",
       " ('喊出', 1),\n",
       " ('本命年', 1),\n",
       " ('自费', 1),\n",
       " ('秋裤', 1),\n",
       " ('集中地', 1),\n",
       " ('本省', 1),\n",
       " ('ト', 1),\n",
       " ('含淚', 1),\n",
       " ('廉政', 1),\n",
       " ('gorgeous', 1),\n",
       " ('一场春梦', 1),\n",
       " ('大难', 1),\n",
       " ('湖南人', 1),\n",
       " ('执法者', 1),\n",
       " ('打胎', 1),\n",
       " ('明与春娇', 1),\n",
       " ('奸雄', 1),\n",
       " ('三片', 1),\n",
       " ('沧海', 1),\n",
       " ('扶桑', 1),\n",
       " ('萌妹', 1),\n",
       " ('社会规范', 1),\n",
       " ('污泥', 1),\n",
       " ('轻缓', 1),\n",
       " ('性本恶', 1),\n",
       " ('新纪元', 1),\n",
       " ('陈静', 1),\n",
       " ('金燕西', 1),\n",
       " ('travel', 1),\n",
       " ('男主得', 1),\n",
       " ('逼会', 1),\n",
       " ('完快', 1),\n",
       " ('庄周', 1),\n",
       " ('刮风', 1),\n",
       " ('海阔天空', 1),\n",
       " ('很二逼', 1),\n",
       " ('每套', 1),\n",
       " ('可到', 1),\n",
       " ('电死', 1),\n",
       " ('洗碗', 1),\n",
       " ('Great', 1),\n",
       " ('适者生存', 1),\n",
       " ('真惨', 1),\n",
       " ('甲壳虫', 1),\n",
       " ('FC', 1),\n",
       " ('FTP', 1),\n",
       " ('孙策', 1),\n",
       " ('马英九', 1),\n",
       " ('成大器', 1),\n",
       " ('板牙', 1),\n",
       " ('黑影', 1),\n",
       " ('丰富性', 1),\n",
       " ('洒热血', 1),\n",
       " ('面看', 1),\n",
       " ('雅芝', 1),\n",
       " ('重慶', 1),\n",
       " ('飞是', 1),\n",
       " ('深地', 1),\n",
       " ('浮浮', 1),\n",
       " ('绘声绘色', 1),\n",
       " ('融在', 1),\n",
       " ('氣味', 1),\n",
       " ('僵化', 1),\n",
       " ('常去', 1),\n",
       " ('对比度', 1),\n",
       " ('算命', 1),\n",
       " ('逍遥法外', 1),\n",
       " ('古裝片', 1),\n",
       " ('全用', 1),\n",
       " ('大能', 1),\n",
       " ('深交', 1),\n",
       " ('洛斯', 1),\n",
       " ('比尔盖茨', 1),\n",
       " ('搭进去', 1),\n",
       " ('露底', 1),\n",
       " ('双唇', 1),\n",
       " ('婚嫁', 1),\n",
       " ('男主鸟', 1),\n",
       " ('慈祥', 1),\n",
       " ('披肩', 1),\n",
       " ('却弱', 1),\n",
       " ('没长', 1),\n",
       " ('别装', 1),\n",
       " ('教会学校', 1),\n",
       " ('蛰伏', 1),\n",
       " ('GOOGLE', 1),\n",
       " ('神幻', 1),\n",
       " ('那罐', 1),\n",
       " ('笑成', 1),\n",
       " ('猫爪', 1),\n",
       " ('赵文暄', 1),\n",
       " ('鳄鱼泪', 1),\n",
       " ('滴大', 1),\n",
       " ('严重破坏', 1),\n",
       " ('六亲不认', 1),\n",
       " ('ticket', 1),\n",
       " ('sucks', 1),\n",
       " ('呛着', 1),\n",
       " ('硬塞', 1),\n",
       " ('运势', 1),\n",
       " ('过命', 1),\n",
       " ('人带', 1),\n",
       " ('我远', 1),\n",
       " ('军旅', 1),\n",
       " ('轮到', 1),\n",
       " ('打不动', 1),\n",
       " ('带伤', 1),\n",
       " ('张世豪', 1),\n",
       " ('拉姆', 1),\n",
       " ('颗粒', 1),\n",
       " ('同声', 1),\n",
       " ('爱达', 1),\n",
       " ('爱豆导', 1),\n",
       " ('花招', 1),\n",
       " ('dodo', 1),\n",
       " ('松林', 1),\n",
       " ('一个十四岁', 1),\n",
       " ('我魂', 1),\n",
       " ('講到', 1),\n",
       " ('海信', 1),\n",
       " ('瘦死', 1),\n",
       " ('情真意切', 1),\n",
       " ('Colter', 1),\n",
       " ('左轮手枪', 1),\n",
       " ('一两秒', 1),\n",
       " ('total', 1),\n",
       " ('亿年', 1),\n",
       " ('旷日持久', 1),\n",
       " ('飞盘', 1),\n",
       " ('薛凱琪', 1),\n",
       " ('周俊伟', 1),\n",
       " ('会动', 1),\n",
       " ('jason', 1),\n",
       " ('龙且', 1),\n",
       " ('阿萨', 1),\n",
       " ('古镇', 1),\n",
       " ('三倍', 1),\n",
       " ('唐纳', 1),\n",
       " ('想尽办法', 1),\n",
       " ('拆得', 1),\n",
       " ('小邓', 1),\n",
       " ('家来', 1),\n",
       " ('菲兹', 1),\n",
       " ('爆泪点', 1),\n",
       " ('部要', 1),\n",
       " ('选它', 1),\n",
       " ('甩掉', 1),\n",
       " ('胸有成竹', 1),\n",
       " ('憨厚老实', 1),\n",
       " ('军训', 1),\n",
       " ('河南人', 1),\n",
       " ('忻', 1),\n",
       " ('knows', 1),\n",
       " ('坠崖', 1),\n",
       " ('温室', 1),\n",
       " ('EN', 1),\n",
       " ('仆', 1),\n",
       " ('河畔', 1),\n",
       " ('然後再', 1),\n",
       " ('演讲时', 1),\n",
       " ('悲摧', 1),\n",
       " ('剩个', 1),\n",
       " ('essay', 1),\n",
       " ('自定义', 1),\n",
       " ('米基', 1),\n",
       " ('周哥', 1),\n",
       " ('永不言败', 1),\n",
       " ('將錯', 1),\n",
       " ('太演', 1),\n",
       " ('触角', 1),\n",
       " ('轶事', 1),\n",
       " ('逢生', 1),\n",
       " ('竞赛', 1),\n",
       " ('上台', 1),\n",
       " ('骸骨', 1),\n",
       " ('人尽可夫', 1),\n",
       " ('遇鬼', 1),\n",
       " ('发家致富', 1),\n",
       " ('按钮', 1),\n",
       " ('靠窗', 1),\n",
       " ('吉吉', 1),\n",
       " ('很亏', 1),\n",
       " ('撥', 1),\n",
       " ('杆子', 1),\n",
       " ('证据不足', 1),\n",
       " ('互杀', 1),\n",
       " ('始自', 1),\n",
       " ('打印机', 1),\n",
       " ('逃避现实', 1),\n",
       " ('外科', 1),\n",
       " ('当阿宝', 1),\n",
       " ('就领', 1),\n",
       " ('許鞍華', 1),\n",
       " ('难记', 1),\n",
       " ('舟', 1),\n",
       " ('疯涨', 1),\n",
       " ('Peace', 1),\n",
       " ('铅华', 1),\n",
       " ('担不起', 1),\n",
       " ('巫太', 1),\n",
       " ('超哥', 1),\n",
       " ('截屏', 1),\n",
       " ('堔', 1),\n",
       " ('春夏', 1),\n",
       " ('怀表', 1),\n",
       " ('CUTE', 1),\n",
       " ('而令', 1),\n",
       " ('垒球', 1),\n",
       " ('心窝子', 1),\n",
       " ('忠于职守', 1),\n",
       " ('白雪皑皑', 1),\n",
       " ('乍现', 1),\n",
       " ('劝诫', 1),\n",
       " ('名士', 1),\n",
       " ('只选', 1),\n",
       " ('党和国家', 1),\n",
       " ('会烂', 1),\n",
       " ('同人剧', 1),\n",
       " ('飘舞', 1),\n",
       " ('咬碎', 1),\n",
       " ('主观性', 1),\n",
       " ('鼓点', 1),\n",
       " ('小鸭', 1),\n",
       " ('爹片', 1),\n",
       " ('丑点', 1),\n",
       " ('可行性', 1),\n",
       " ('不逊于', 1),\n",
       " ('参照物', 1),\n",
       " ('家暴', 1),\n",
       " ('接得', 1),\n",
       " ('猴', 1),\n",
       " ('耻辱柱', 1),\n",
       " ('睡上', 1),\n",
       " ('两手', 1),\n",
       " ('狗辈', 1),\n",
       " ('边有', 1),\n",
       " ('換了', 1),\n",
       " ('梅家', 1),\n",
       " ('宝刀', 1),\n",
       " ('Gabby', 1),\n",
       " ('不来赛', 1),\n",
       " ('impossible', 1),\n",
       " ('举棋不定', 1),\n",
       " ('插件', 1),\n",
       " ('别一', 1),\n",
       " ('Tree', 1),\n",
       " ('带回去', 1),\n",
       " ('怯生生', 1),\n",
       " ('片约', 1),\n",
       " ('七零八碎', 1),\n",
       " ('费尽', 1),\n",
       " ('喟叹', 1),\n",
       " ('george', 1),\n",
       " ('绝学', 1),\n",
       " ('遨游', 1),\n",
       " ('完来', 1),\n",
       " ('五种', 1),\n",
       " ('不唱', 1),\n",
       " ('星梦', 1),\n",
       " ('喝茶', 1),\n",
       " ('动手术', 1),\n",
       " ('前向', 1),\n",
       " ('有知', 1),\n",
       " ('又立', 1),\n",
       " ('戲里', 1),\n",
       " ('纠集', 1),\n",
       " ('producer', 1),\n",
       " ('卡瓦', 1),\n",
       " ('红军', 1),\n",
       " ('嘖', 1),\n",
       " ('巩利', 1),\n",
       " ('舞剧', 1),\n",
       " ('菲利浦', 1),\n",
       " ('清迈', 1),\n",
       " ('某年', 1),\n",
       " ('铲除', 1),\n",
       " ('举措', 1),\n",
       " ('要造', 1),\n",
       " ('通缉犯', 1),\n",
       " ('地裂', 1),\n",
       " ('将才', 1),\n",
       " ('亿美元', 1),\n",
       " ('漫无止境', 1),\n",
       " ('溏', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 句子最大长度\n",
    "SENTENCE_LIMIT_SIZE = 50\n",
    "# 统计词汇\n",
    "train_total_w = train_pos_w + train_neg_w\n",
    "validation_total_w = validation_pos_w + validation_neg_w\n",
    "test_total_w = test_pos_w + test_neg_w\n",
    "train_c = Counter(train_total_w)\n",
    "validation_c = Counter(validation_total_w)\n",
    "test_c = Counter(test_total_w)\n",
    "# 倒序查看词频\n",
    "sorted(train_c.most_common(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化两个token：pad和unk\n",
    "train_vocab = [\"<pad>\", \"<unk>\"]\n",
    "validation_vocab = [\"<pad>\", \"<unk>\"]\n",
    "test_vocab = [\"<pad>\", \"<unk>\"]\n",
    "# 去除出现频次为1次的单词\n",
    "for w, f in train_c.most_common():\n",
    "    if f > 1:\n",
    "        train_vocab.append(w)\n",
    "for w, f in validation_c.most_common():\n",
    "    if f > 1:\n",
    "        validation_vocab.append(w)\n",
    "for w, f in test_c.most_common():\n",
    "    if f > 1:\n",
    "        test_vocab.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of our vocabulary is: 32199\n"
     ]
    }
   ],
   "source": [
    "print(\"The total size of our vocabulary is: {}\".format(len(train_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单词到编码的映射，例如machine -> 10283\n",
    "train_word_to_token = {word: token for token, word in enumerate(train_vocab)}\n",
    "validation_word_to_token = {word: token for token, word in enumerate(validation_vocab)}\n",
    "test_word_to_token = {word: token for token, word in enumerate(test_vocab)}\n",
    "# 编码到单词的映射，例如10283 -> machine\n",
    "train_token_to_word = {token: word for word, token in train_word_to_token.items()}\n",
    "validation_token_to_word = {token: word for word, token in validation_word_to_token.items()}\n",
    "test_token_to_word = {token: word for word, token in test_word_to_token.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_token(sentence, word_to_token_map, limit_size=SENTENCE_LIMIT_SIZE):\n",
    "    \"\"\"\n",
    "    根据单词-编码映射表将单个句子转化为token\n",
    "    \n",
    "    @param sentence: 句子，str类型\n",
    "    @param word_to_token_map: 单词到编码的映射\n",
    "    @param limit_size: 句子最大长度。超过该长度的句子进行截断，不足的句子进行pad补全\n",
    "    \n",
    "    return: 句子转换为token后的列表\n",
    "    \"\"\"\n",
    "    # 获取unknown单词和pad的token\n",
    "    unk_id = word_to_token_map[\"<unk>\"]\n",
    "    pad_id = word_to_token_map[\"<pad>\"]\n",
    "    \n",
    "    # 对句子进行token转换，对于未在词典中出现过的词用unk的token填充\n",
    "    tokens = [word_to_token_map.get(word, unk_id) for word in sentence.lower().split()]\n",
    "    \n",
    "    # Pad\n",
    "    if len(tokens) < limit_size:\n",
    "        tokens.extend([0] * (limit_size - len(tokens)))\n",
    "    # Trunc\n",
    "    else:\n",
    "        tokens = tokens[:limit_size]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 40798.77it/s]\n",
      "100%|██████████| 2812/2812 [00:00<00:00, 15633.17it/s]\n",
      "100%|██████████| 187/187 [00:00<00:00, 34933.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# 对pos文本处理\n",
    "train_pos_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(train_pos_text):\n",
    "    tokens = convert_text_to_token(sentence, train_word_to_token)\n",
    "    train_pos_tokens.append(tokens)\n",
    "\n",
    "validation_pos_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(validation_pos_text):\n",
    "    tokens = convert_text_to_token(sentence, validation_word_to_token)\n",
    "    validation_pos_tokens.append(tokens)\n",
    "\n",
    "test_pos_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(test_pos_text):\n",
    "    tokens = convert_text_to_token(sentence, test_word_to_token)\n",
    "    test_pos_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 39317.99it/s]\n",
      "100%|██████████| 2817/2817 [00:00<00:00, 42234.50it/s]\n",
      "100%|██████████| 185/185 [00:00<00:00, 41020.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# 对neg文本处理\n",
    "train_neg_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(train_neg_text):\n",
    "    tokens = convert_text_to_token(sentence, train_word_to_token)\n",
    "    train_neg_tokens.append(tokens)\n",
    "\n",
    "validation_neg_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(validation_neg_text):\n",
    "    tokens = convert_text_to_token(sentence, validation_word_to_token)\n",
    "    validation_neg_tokens.append(tokens)\n",
    "\n",
    "test_neg_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(test_neg_text):\n",
    "    tokens = convert_text_to_token(sentence, test_word_to_token)\n",
    "    test_neg_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为numpy格式，方便处理\n",
    "train_pos_tokens = np.array(train_pos_tokens)\n",
    "train_neg_tokens = np.array(train_neg_tokens)\n",
    "validation_pos_tokens = np.array(validation_pos_tokens)\n",
    "validation_neg_tokens = np.array(validation_neg_tokens)\n",
    "test_pos_tokens = np.array(test_pos_tokens)\n",
    "test_neg_tokens = np.array(test_neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有语料\n",
    "train_total_tokens = np.concatenate((train_pos_tokens, train_neg_tokens), axis=0)\n",
    "validation_total_tokens = np.concatenate((validation_pos_tokens, validation_neg_tokens), axis=0)\n",
    "test_total_tokens = np.concatenate((test_pos_tokens, test_neg_tokens), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of all tokens in our corpus: (19998, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of all tokens in our corpus: ({}, {})\".format(*train_total_tokens.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为numpy格式，方便处理\n",
    "train_pos_targets = np.ones((train_pos_tokens.shape[0]))\n",
    "train_neg_targets = np.zeros((train_neg_tokens.shape[0]))\n",
    "validation_pos_targets = np.ones((validation_pos_tokens.shape[0]))\n",
    "validation_neg_targets = np.zeros((validation_neg_tokens.shape[0]))\n",
    "test_pos_targets = np.ones((test_pos_tokens.shape[0]))\n",
    "test_neg_targets = np.zeros((test_neg_tokens.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有target\n",
    "train_total_targets = np.concatenate((train_pos_targets, train_neg_targets), axis=0).reshape(-1, 1)\n",
    "validation_total_targets = np.concatenate((validation_pos_targets, validation_neg_targets), axis=0).reshape(-1, 1)\n",
    "test_total_targets = np.concatenate((test_pos_targets, test_neg_targets), axis=0).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of all targets in our corpus: (19998, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of all targets in our corpus: ({}, {})\".format(*train_total_targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18676    55 15999 31601    55 28079    45    55   218   459    65   899\n",
      "   287   535  8082   152 12850    65     1     1     2    47    27 21304\n",
      "    61   110    16 28088   336     1  2784  3201    84  3187     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_pos_tokens[0])\n",
    "print(train_pos_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_total_tokens\n",
    "y_train = train_total_targets\n",
    "x_validation = validation_total_tokens\n",
    "y_validation = validation_total_targets\n",
    "x_test = test_total_tokens\n",
    "y_test = test_total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "n_words = len(train_vocab) + 1\n",
    "fname = './Dataset/wiki_word2vec_50.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))\n",
    "# print(word_vecs)\n",
    "for word in train_word_to_token.keys():\n",
    "    try:\n",
    "        word_vecs[train_word_to_token[word]] = model[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "# 重置PAD为0向量\n",
    "static_embeddings = word_vecs\n",
    "pad_id = train_word_to_token[\"<pad>\"]\n",
    "static_embeddings[pad_id, :] = np.zeros(len(word_vecs[0]))\n",
    "static_embeddings = static_embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.5302895  -0.14834027  0.39980638 ... -0.45743927  0.8373424\n",
      "  -0.85130596]\n",
      " [ 0.36876917 -0.14839791  0.00680677 ... -0.5649007   0.34907597\n",
      "  -0.5528553 ]\n",
      " ...\n",
      " [ 0.06381914 -0.24131483 -0.2453062  ... -0.5496541   1.0106983\n",
      "  -0.0432074 ]\n",
      " [ 0.5275323  -0.32757896  0.5682959  ... -0.62072605  0.90837115\n",
      "   0.00559371]\n",
      " [ 0.16874874 -0.0734352  -0.01415925 ... -0.4555091   0.4978862\n",
      "   0.27871725]]\n"
     ]
    }
   ],
   "source": [
    "print(len(static_embeddings[0]))\n",
    "print(static_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空图\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 我在这里定义了5种filter，每种100个\n",
    "filters_size = [2, 3, 4, 5, 6]\n",
    "num_filters = 100\n",
    "# 超参数\n",
    "BATCH_SIZE = 100\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 0.001\n",
    "L2_LAMBDA = 10\n",
    "KEEP_PROB = 0.5\n",
    "EMBEDDING_SIZE = model.vector_size\n",
    "VOCAB_SIZE = len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    assert x.shape[0] == y.shape[0], print(\"error shape!\")\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        shuffled_index = np.random.permutation(range(x.shape[0]))\n",
    "\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "    \n",
    "    # 统计共几个完整的batch\n",
    "    n_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        x_batch = x[i*batch_size: (i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "    \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-280b9c158bed>:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cnn\"):\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        inputs = tf.placeholder(dtype=tf.int32, shape=(None, 50), name=\"inputs\")\n",
    "        targets = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"targets\")\n",
    "    \n",
    "    # embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding_matrix = tf.Variable(initial_value=static_embeddings, trainable=False, name=\"embedding_matrix\")\n",
    "        embed = tf.nn.embedding_lookup(embedding_matrix, inputs, name=\"embed\")\n",
    "        # 添加channel维度\n",
    "        embed_expanded = tf.expand_dims(embed, -1, name=\"embed_expand\")\n",
    "    \n",
    "    # 用来存储max-pooling的结果\n",
    "    pooled_outputs = []\n",
    "\n",
    "    # 迭代多个filter\n",
    "    for i, filter_size in enumerate(filters_size):\n",
    "        with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "            filter_shape = [filter_size, EMBEDDING_SIZE, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.zeros(num_filters), name=\"b\")\n",
    "\n",
    "            conv = tf.nn.conv2d(input=embed_expanded, \n",
    "                                 filter=W, \n",
    "                                 strides=[1, 1, 1, 1], \n",
    "                                 padding=\"VALID\",\n",
    "                                 name=\"conv\")\n",
    "\n",
    "            # 激活\n",
    "            a = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"activations\")\n",
    "            # 池化\n",
    "            max_pooling = tf.nn.max_pool(value=a, \n",
    "                                    ksize=[1, SENTENCE_LIMIT_SIZE - filter_size + 1, 1, 1],\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\",\n",
    "                                    name=\"max_pooling\")\n",
    "            pooled_outputs.append(max_pooling)\n",
    "    \n",
    "    # 统计所有的filter\n",
    "    total_filters = num_filters * len(filters_size)\n",
    "    total_pool = tf.concat(pooled_outputs, 3)\n",
    "    flattend_pool = tf.reshape(total_pool, (-1, total_filters))\n",
    "    \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        dropout = tf.nn.dropout(flattend_pool, KEEP_PROB)\n",
    "    \n",
    "    # output\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\"W\", shape=(total_filters, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(1), name=\"b\")\n",
    "        \n",
    "        logits = tf.add(tf.matmul(dropout, W), b)\n",
    "        predictions = tf.nn.sigmoid(logits, name=\"predictions\")\n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "#         loss = loss + L2_LAMBDA * tf.nn.l2_loss(W)\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    # evaluation\n",
    "    with tf.name_scope(\"evaluation\"):\n",
    "        correct_preds = tf.equal(tf.cast(tf.greater(predictions, 0.5), tf.float32), targets)\n",
    "        accuracy = tf.reduce_sum(tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储准确率\n",
    "cnn_train_accuracy = []\n",
    "cnn_validation_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, Training loss: 0.6333, Train accuracy: 0.7411, Validation accuracy: 0.5052\n",
      "Training epoch: 2, Training loss: 0.5100, Train accuracy: 0.7664, Validation accuracy: 0.5244\n",
      "Training epoch: 3, Training loss: 0.4661, Train accuracy: 0.7964, Validation accuracy: 0.5223\n",
      "Training epoch: 4, Training loss: 0.4354, Train accuracy: 0.8171, Validation accuracy: 0.5257\n",
      "Training epoch: 5, Training loss: 0.4080, Train accuracy: 0.8174, Validation accuracy: 0.5207\n",
      "Training epoch: 6, Training loss: 0.3796, Train accuracy: 0.8471, Validation accuracy: 0.5262\n",
      "Training epoch: 7, Training loss: 0.3577, Train accuracy: 0.8568, Validation accuracy: 0.5175\n",
      "Training epoch: 8, Training loss: 0.3370, Train accuracy: 0.8724, Validation accuracy: 0.5109\n",
      "Training epoch: 9, Training loss: 0.3170, Train accuracy: 0.8880, Validation accuracy: 0.5195\n",
      "Training epoch: 10, Training loss: 0.2940, Train accuracy: 0.8968, Validation accuracy: 0.5255\n",
      "Training epoch: 11, Training loss: 0.2817, Train accuracy: 0.9069, Validation accuracy: 0.5237\n",
      "Training epoch: 12, Training loss: 0.2551, Train accuracy: 0.9161, Validation accuracy: 0.5127\n",
      "Training epoch: 13, Training loss: 0.2390, Train accuracy: 0.9199, Validation accuracy: 0.5107\n",
      "Training epoch: 14, Training loss: 0.2264, Train accuracy: 0.9263, Validation accuracy: 0.5198\n",
      "Training epoch: 15, Training loss: 0.2162, Train accuracy: 0.9308, Validation accuracy: 0.5184\n",
      "Training epoch: 16, Training loss: 0.2009, Train accuracy: 0.9398, Validation accuracy: 0.5198\n",
      "Training epoch: 17, Training loss: 0.1892, Train accuracy: 0.9444, Validation accuracy: 0.5250\n",
      "Training epoch: 18, Training loss: 0.1737, Train accuracy: 0.9426, Validation accuracy: 0.5182\n",
      "Training epoch: 19, Training loss: 0.1701, Train accuracy: 0.9487, Validation accuracy: 0.5154\n",
      "Training epoch: 20, Training loss: 0.1642, Train accuracy: 0.9511, Validation accuracy: 0.5091\n",
      "Training epoch: 21, Training loss: 0.1548, Train accuracy: 0.9495, Validation accuracy: 0.5111\n",
      "Training epoch: 22, Training loss: 0.1431, Train accuracy: 0.9523, Validation accuracy: 0.5125\n",
      "Training epoch: 23, Training loss: 0.1404, Train accuracy: 0.9566, Validation accuracy: 0.5189\n",
      "Training epoch: 24, Training loss: 0.1379, Train accuracy: 0.9603, Validation accuracy: 0.5147\n",
      "Training epoch: 25, Training loss: 0.1312, Train accuracy: 0.9611, Validation accuracy: 0.5175\n",
      "Training epoch: 26, Training loss: 0.1285, Train accuracy: 0.9484, Validation accuracy: 0.5141\n",
      "Training epoch: 27, Training loss: 0.1212, Train accuracy: 0.9627, Validation accuracy: 0.5225\n",
      "Training epoch: 28, Training loss: 0.1115, Train accuracy: 0.9646, Validation accuracy: 0.5102\n",
      "Training epoch: 29, Training loss: 0.1119, Train accuracy: 0.9662, Validation accuracy: 0.5198\n",
      "Training epoch: 30, Training loss: 0.1072, Train accuracy: 0.9686, Validation accuracy: 0.5131\n",
      "Training epoch: 31, Training loss: 0.1096, Train accuracy: 0.9675, Validation accuracy: 0.5193\n",
      "Training epoch: 32, Training loss: 0.1015, Train accuracy: 0.9715, Validation accuracy: 0.5161\n",
      "Training epoch: 33, Training loss: 0.0970, Train accuracy: 0.9692, Validation accuracy: 0.5141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4f6c226f19b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             _, l = sess.run([optimizer, loss],\n\u001b[1;32m     11\u001b[0m                             feed_dict={inputs: x_batch, \n\u001b[0;32m---> 12\u001b[0;31m                                        targets: y_batch})\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./graphs/cnn\", tf.get_default_graph())\n",
    "    n_batches = int(x_train.shape[0] / BATCH_SIZE)\n",
    "    \n",
    "    for epoch in range(EPOCHES):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in get_batch(x_train, y_train):\n",
    "            _, l = sess.run([optimizer, loss],\n",
    "                            feed_dict={inputs: x_batch, \n",
    "                                       targets: y_batch})\n",
    "            total_loss += l\n",
    "        \n",
    "        train_corrects = sess.run(accuracy, feed_dict={inputs: x_train, targets: y_train})\n",
    "        train_acc = train_corrects / x_train.shape[0]\n",
    "        cnn_train_accuracy.append(train_acc)\n",
    "        \n",
    "        validation_corrects = sess.run(accuracy, feed_dict={inputs: x_validation, targets: y_validation})\n",
    "        validation_acc = validation_corrects / x_validation.shape[0]\n",
    "        cnn_validation_accuracy.append(validation_acc)\n",
    "        \n",
    "        print(\"Training epoch: {}, Training loss: {:.4f}, Train accuracy: {:.4f}, Validation accuracy: {:.4f}\"\n",
    "              .format(epoch + 1, total_loss / n_batches, train_acc, validation_acc))\n",
    "    test_corrects = sess.run(accuracy, feed_dict={inputs: x_test, targets: y_test})\n",
    "    test_acc = test_corrects / x_test.shape[0]\n",
    "    \n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))                                                                                                            \n",
    "                                                                                                                \n",
    "    saver.save(sess, \"checkpoints/cnn\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(cnn_train_accuracy)\n",
    "plt.plot(cnn_validation_accuracy)\n",
    "plt.ylim(ymin=0.5, ymax=1.01)\n",
    "plt.title(\"The accuracy of CNN model\")\n",
    "plt.legend([\"train\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空图\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"cnn_multichannels\"):\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        inputs = tf.placeholder(dtype=tf.int32, shape=(None, 20), name=\"inputs\")\n",
    "        targets = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"targets\")\n",
    "    # embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        # static embeddings\n",
    "        static_embedding_matrix = tf.Variable(initial_value=static_embeddings, \n",
    "                                              trainable=False, \n",
    "                                              name=\"static_embedding_matrix\")\n",
    "        static_embed = tf.nn.embedding_lookup(static_embedding_matrix, inputs, name=\"static_embed\")\n",
    "        static_embed_expanded = tf.expand_dims(static_embed, -1, name=\"static_embed_expand\")\n",
    "        \n",
    "        # non-static embeddings\n",
    "        dynamic_embedding_matrix = tf.Variable(tf.random_normal(shape=(VOCAB_SIZE, EMBEDDING_SIZE), stddev=0.1), \n",
    "                                               trainable=True, \n",
    "                                               name=\"dynamic_embedding_matrix\")\n",
    "        dynamic_embed = tf.nn.embedding_lookup(dynamic_embedding_matrix, inputs, name=\"dynamic_embed\")\n",
    "        dynamic_embed_expanded = tf.expand_dims(dynamic_embed, -1, name=\"dynamic_embed_expand\")\n",
    "        \n",
    "        # stack\n",
    "        embed_expanded = tf.concat((static_embed_expanded, dynamic_embed_expanded), axis=-1, name=\"embed_expanded\")\n",
    "    \n",
    "    pooled_outputs = []\n",
    "\n",
    "    # 迭代多个filter\n",
    "    for i, filter_size in enumerate(filters_size):\n",
    "        with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "            # 注意这里filter的channel要指定为2\n",
    "            filter_shape = [filter_size, EMBEDDING_SIZE, 2, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.zeros(num_filters), name=\"b\")\n",
    "\n",
    "            conv = tf.nn.conv2d(input=embed_expanded, \n",
    "                                 filter=W, \n",
    "                                 strides=[1, 1, 1, 1], \n",
    "                                 padding=\"VALID\",\n",
    "                                 name=\"conv\")\n",
    "\n",
    "            # 激活\n",
    "            a = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"activations\")\n",
    "            # 池化\n",
    "            max_pooling = tf.nn.max_pool(value=a, \n",
    "                                    ksize=[1, SENTENCE_LIMIT_SIZE - filter_size + 1, 1, 1],\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                   padding=\"VALID\",\n",
    "                                   name=\"max_pooling\")\n",
    "            pooled_outputs.append(max_pooling)\n",
    "        \n",
    "    total_filters = num_filters * len(filters_size)\n",
    "    total_pool = tf.concat(pooled_outputs, 3)\n",
    "    flattend_pool = tf.reshape(total_pool, (-1, total_filters))\n",
    "    \n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        dropout = tf.nn.dropout(flattend_pool, KEEP_PROB)\n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\"W\", shape=(total_filters, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(1), name=\"b\")\n",
    "        \n",
    "        logits = tf.add(tf.matmul(dropout, W), b)\n",
    "        predictions = tf.nn.sigmoid(logits, name=\"predictions\")\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "        loss = loss + L2_LAMBDA * tf.nn.l2_loss(W)\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope(\"evaluation\"):\n",
    "        correct_preds = tf.equal(tf.cast(tf.greater(predictions, 0.5), tf.float32), targets)\n",
    "        accuracy = tf.reduce_sum(tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cnn_train_accuracy = []\n",
    "multi_cnn_validation_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 10\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./graphs/multi_cnn\", tf.get_default_graph())\n",
    "    n_batches = int(x_train.shape[0] / BATCH_SIZE)\n",
    "    \n",
    "    for epoch in range(EPOCHES):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in get_batch(x_train, y_train):\n",
    "            _, l = sess.run([optimizer, loss],\n",
    "                            feed_dict={inputs: x_batch, \n",
    "                                       targets: y_batch})\n",
    "            total_loss += l\n",
    "        \n",
    "        train_corrects = sess.run(accuracy, feed_dict={inputs: x_train, targets: y_train})\n",
    "        train_acc = train_corrects / x_train.shape[0]\n",
    "        multi_cnn_train_accuracy.append(train_acc)\n",
    "        \n",
    "        validation_corrects = sess.run(accuracy, feed_dict={inputs: x_validation, targets: y_validation})\n",
    "        validation_acc = validation_corrects / x_validation.shape[0]\n",
    "        multi_cnn_validation_accuracy.append(validation_acc)\n",
    "        \n",
    "        print(\"Training epoch: {}, Training loss: {:.4f}, Train accuracy: {:.4f}, Validation accuracy: {:.4f}\".format(epoch + 1, \n",
    "                                                                                                                total_loss / n_batches,\n",
    "                                                                                                                train_acc,\n",
    "                                                                                                                validation_acc))\n",
    "    \n",
    "    \n",
    "    test_corrects = sess.run(accuracy, feed_dict={inputs: x_test, targets: y_test})\n",
    "    test_acc = test_corrects / x_test.shape[0]\n",
    "    \n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc)) \n",
    "    \n",
    "    saver.save(sess, \"checkpoints/multi_cnn\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(multi_cnn_train_accuracy)\n",
    "# plt.plot(multi_cnn_validation_accuracy)\n",
    "# plt.ylim(ymin=0.5, ymax=1.01)\n",
    "# plt.title(\"The accuracy of multi-channel CNN model\")\n",
    "# plt.legend([\"train\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
