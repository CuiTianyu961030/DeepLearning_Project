{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from PIL import Image\n",
    "\n",
    "SIZE = 400\n",
    "WIDTH = 20\n",
    "HEIGHT = 20\n",
    "NUM_CLASSES = 31\n",
    "iterations = 300\n",
    "\n",
    "SAVER_DIR_P = \"train-saver/province/\"\n",
    "SAVER_DIR_D = \"train-saver/digits/\"\n",
    "SAVER_DIR_L = \"train-saver/letters/\"\n",
    "\n",
    "\n",
    "PROVINCES = (\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \n",
    "             \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\")\n",
    "\n",
    "LETTERS_DIGITS = (\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"J\",\"K\",\n",
    "                  \"L\",\"M\",\"N\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\")\n",
    "\n",
    "LETTERS = (\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\n",
    "                  \"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\")\n",
    "\n",
    "license_num = \"\"\n",
    "\n",
    "nProvinceIndex = 0\n",
    "\n",
    "time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "input_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/province/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            input_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "input_images = np.array([[0]*SIZE for i in range(input_count)])\n",
    "input_labels = np.array([[0]*NUM_CLASSES for i in range(input_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/province/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "#         print (\"省份简称是: %s\" % PROVINCES[i])\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    input_images[index][w+h*width] = img.getpixel((w, h))\n",
    "#                 testimg = np.reshape(input_images[index], (20,20))\n",
    "#                 print(testimg)\n",
    "            input_labels[index][i] = 1\n",
    "#                 print(input_labels[index])\n",
    "            index += 1\n",
    "\n",
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "val_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/province/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            val_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "val_images = np.array([[0]*SIZE for i in range(val_count)])\n",
    "val_labels = np.array([[0]*NUM_CLASSES for i in range(val_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/province/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "#         print (\"省份简称是: %s\" % PROVINCES[i])\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    val_images[index][w+h*width] = img.getpixel((w, h))\n",
    "            val_labels[index][i] = 1\n",
    "#                 print(i)\n",
    "#                 print(val_labels[index])\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-7e9d9e22fa1a>:43: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-7e9d9e22fa1a>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def conv_layer(inputs, W, b, conv_strides, kernel_size, pool_strides, padding):\n",
    "    L1_conv = tf.nn.conv2d(inputs, W, strides=conv_strides, padding=padding)\n",
    "    L1_relu = tf.nn.relu(L1_conv + b)\n",
    "    return tf.nn.max_pool(L1_relu, ksize=kernel_size, strides=pool_strides, padding='SAME')\n",
    " \n",
    "\n",
    "# 定义全连接层函数\n",
    "def full_connect(inputs, W, b):\n",
    "    return tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, SIZE])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, WIDTH, HEIGHT, 1])\n",
    "\n",
    "# 第一个卷积层\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, 16], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[16]), name=\"b_conv1\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 2, 2, 1]\n",
    "pool_strides = [1, 2, 2, 1]\n",
    "L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "# 第二个卷积层\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([3, 3, 16, 32], stddev=0.1), name=\"W_conv2\")\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[32]), name=\"b_conv2\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 1, 1, 1]\n",
    "pool_strides = [1, 1, 1, 1]\n",
    "L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "# 全连接层\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([10 * 10 * 32, 512], stddev=0.1), name=\"W_fc1\")\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[512]), name=\"b_fc1\")\n",
    "h_pool2_flat = tf.reshape(L2_pool, [-1, 10 * 10 * 32])\n",
    "h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "# readout层\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([512, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "# 定义优化器和训练op\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次训练迭代: train 准确率 15.72508%，val准确率 13.68421%\n",
      "第 5 次训练迭代: train 准确率 43.03706%，val准确率 37.63158%\n",
      "第 10 次训练迭代: train 准确率 58.94207%，val准确率 56.31579%\n",
      "第 15 次训练迭代: train 准确率 70.92479%，val准确率 70.26316%\n",
      "第 20 次训练迭代: train 准确率 80.71249%，val准确率 80.78948%\n",
      "第 25 次训练迭代: train 准确率 85.57035%，val准确率 84.21053%\n",
      "第 30 次训练迭代: train 准确率 88.70097%，val准确率 87.89474%\n",
      "第 35 次训练迭代: train 准确率 91.43577%，val准确率 90.00000%\n",
      "第 40 次训练迭代: train 准确率 93.19899%，val准确率 91.84210%\n",
      "第 45 次训练迭代: train 准确率 94.60238%，val准确率 92.63158%\n",
      "第 50 次训练迭代: train 准确率 95.46599%，val准确率 93.68421%\n",
      "第 55 次训练迭代: train 准确率 96.18568%，val准确率 94.73684%\n",
      "第 60 次训练迭代: train 准确率 96.65347%，val准确率 94.47368%\n",
      "第 65 次训练迭代: train 准确率 97.26520%，val准确率 95.00000%\n",
      "第 70 次训练迭代: train 准确率 97.98489%，val准确率 95.26316%\n",
      "第 75 次训练迭代: train 准确率 98.45268%，val准确率 95.26316%\n",
      "第 80 次训练迭代: train 准确率 98.92048%，val准确率 95.00000%\n",
      "第 85 次训练迭代: train 准确率 99.17237%，val准确率 95.00000%\n",
      "第 90 次训练迭代: train 准确率 99.31630%，val准确率 95.78947%\n",
      "第 95 次训练迭代: train 准确率 99.38827%，val准确率 96.31579%\n",
      "第 100 次训练迭代: train 准确率 99.53220%，val准确率 96.57895%\n",
      "第 105 次训练迭代: train 准确率 99.64016%，val准确率 96.31579%\n",
      "第 110 次训练迭代: train 准确率 99.71213%，val准确率 97.10526%\n",
      "第 115 次训练迭代: train 准确率 99.82008%，val准确率 96.84210%\n",
      "第 120 次训练迭代: train 准确率 99.82008%，val准确率 97.10526%\n",
      "第 125 次训练迭代: train 准确率 99.92803%，val准确率 96.84210%\n",
      "第 130 次训练迭代: train 准确率 99.92803%，val准确率 96.84210%\n",
      "第 135 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 140 次训练迭代: train 准确率 99.96402%，val准确率 96.84210%\n",
      "第 145 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 150 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 155 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 160 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 165 次训练迭代: train 准确率 99.96402%，val准确率 96.84210%\n",
      "第 170 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 175 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 180 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 185 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 190 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 195 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 200 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 205 次训练迭代: train 准确率 99.96402%，val准确率 97.89473%\n",
      "第 210 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 215 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 220 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 225 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 230 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 235 次训练迭代: train 准确率 99.96402%，val准确率 98.15789%\n",
      "第 240 次训练迭代: train 准确率 99.96402%，val准确率 96.84210%\n",
      "第 245 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 250 次训练迭代: train 准确率 99.96402%，val准确率 97.10526%\n",
      "第 255 次训练迭代: train 准确率 99.96402%，val准确率 97.89473%\n",
      "第 260 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 265 次训练迭代: train 准确率 99.96402%，val准确率 97.89473%\n",
      "第 270 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 275 次训练迭代: train 准确率 99.96402%，val准确率 97.63158%\n",
      "第 280 次训练迭代: train 准确率 99.96402%，val准确率 96.84210%\n",
      "第 285 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 290 次训练迭代: train 准确率 99.96402%，val准确率 97.36842%\n",
      "第 295 次训练迭代: train 准确率 99.96402%，val准确率 97.89473%\n",
      "训练耗费时间：1291秒\n",
      "\n",
      "预测：./test_images/1510076148_823_1.bmp\n",
      "概率：  [京 99.96%]    [琼 0.03%]    [宁 0.00%]\n",
      "省份简称是: 京\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        # 初始化saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    time_elapsed = time.time() - time_begin\n",
    "#     print(\"读取图片文件耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "#     print (\"一共读取了 %s 个训练图像， %s 个标签\" % (input_count, input_count))\n",
    "#     print (\"一共读取了 %s 个验证图像， %s 个标签\" % (val_count, val_count))\n",
    "\n",
    "#     print (\"train shape:\", (input_images.shape))\n",
    "#     print (\"val shape:\", (val_images.shape))\n",
    "\n",
    "\n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 60\n",
    "    iterations = iterations\n",
    "    batches_count = int(input_count / batch_size)\n",
    "    remainder = input_count % batch_size\n",
    "#     print (\"训练数据集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count+1, batch_size, remainder))\n",
    "#     print(input_images.shape)\n",
    "#     print(val_images.shape)\n",
    "    input_images.reshape((-1, 20*20))\n",
    "    val_images.reshape((-1, 20*20))\n",
    "\n",
    "    input_images = input_images.astype(\"float32\")/255\n",
    "    val_images = val_images.astype(\"float32\")/255\n",
    "    testimg = np.reshape(input_images[0], (-1, 20*20))\n",
    "#         testimg = testimg.astype(\"float32\")/255\n",
    "#     print(testimg)\n",
    "    testimg = np.reshape(val_images[0], (-1, 20*20))\n",
    "#         testimg = testimg.astype(\"float32\")/255\n",
    "#     print(testimg)\n",
    "#         print(type(input_images))\n",
    "#         print(type(val_images))\n",
    "#     print(input_images.shape)\n",
    "#     print(val_images.shape)\n",
    "\n",
    "    # 执行训练迭代\n",
    "    for it in range(iterations):\n",
    "        # 这里的关键是要把输入数组转为np.array\n",
    "        for n in range(batches_count):\n",
    "            train_step.run(feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y_: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.5})\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size;\n",
    "            train_step.run(feed_dict={x: input_images[start_index:-1], y_: input_labels[start_index:-1], keep_prob: 0.5})\n",
    "\n",
    "        # 每完成五次迭代，判断准确度是否已达到100%，达到则退出迭代循环\n",
    "        iterate_accuracy = 0\n",
    "        if it%5 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: input_images, y_: input_labels, keep_prob: 1.0})\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: val_images, y_: val_labels, keep_prob: 1.0})\n",
    "            print ('第 %d 次训练迭代: train 准确率 %0.5f%%，val准确率 %0.5f%%' % (it, train_accuracy*100, iterate_accuracy*100))\n",
    "            if iterate_accuracy >= 0.9999 and it >= 150:\n",
    "                break;\n",
    "\n",
    "#     print ('完成训练!')\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print (\"训练耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "    # 保存训练结果\n",
    "    if not os.path.exists(SAVER_DIR_P):\n",
    "        print ('不存在训练数据保存目录，现在创建保存目录')\n",
    "        os.makedirs(SAVER_DIR_P)\n",
    "    saver_path = saver.save(sess, \"%smodel.ckpt\"%(SAVER_DIR_P))\n",
    "    \n",
    "    for n in range(1,2):\n",
    "#             path = \"test_images/%s.bmp\" % (n)\n",
    "        path = os.path.join(\"./test_images\", \"1510076148_823_1.bmp\")\n",
    "        print(\"\\n预测：./test_images/1510076148_823_1.bmp\")\n",
    "        \n",
    "        img = Image.open(path)\n",
    "        img = img.resize((20,20))\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "\n",
    "        img_data = np.array([[0]*SIZE for i in range(1)])\n",
    "        for h in range(0, height):\n",
    "            for w in range(0, width):\n",
    "                img_data[0][w+h*width]=img.getpixel((w,h))\n",
    "#                     if img.getpixel((w, h)) < 190:\n",
    "#                         img_data[0][w+h*width] = 1\n",
    "#                     else:\n",
    "#                         img_data[0][w+h*width] = 0\n",
    "\n",
    "        img = img_data.astype('float32')/255\n",
    "        result = sess.run(conv, feed_dict = {x: img, keep_prob: 1.0})\n",
    "        max1 = 0\n",
    "        max2 = 0\n",
    "        max3 = 0\n",
    "        max1_index = 0\n",
    "        max2_index = 0\n",
    "        max3_index = 0\n",
    "        for j in range(NUM_CLASSES):\n",
    "            if result[0][j] > max1:\n",
    "                max1 = result[0][j]\n",
    "                max1_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max2) and (result[0][j]<=max1):\n",
    "                max2 = result[0][j]\n",
    "                max2_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max3) and (result[0][j]<=max2):\n",
    "                max3 = result[0][j]\n",
    "                max3_index = j\n",
    "                continue\n",
    "\n",
    "        nProvinceIndex = max1_index\n",
    "        print (\"概率：  [%s %0.2f%%]    [%s %0.2f%%]    [%s %0.2f%%]\" % (PROVINCES[max1_index],max1*100, PROVINCES[max2_index],max2*100, PROVINCES[max3_index],max3*100))\n",
    "\n",
    "    print (\"省份简称是: %s\" % PROVINCES[nProvinceIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "input_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/letter/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            input_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "input_images = np.array([[0]*SIZE for i in range(input_count)])\n",
    "input_labels = np.array([[0]*NUM_CLASSES for i in range(input_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/letter/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    input_images[index][w+h*width] = img.getpixel((w,h))\n",
    "            input_labels[index][i] = 1\n",
    "            index += 1\n",
    "\n",
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "val_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/letter/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            val_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "val_images = np.array([[0]*SIZE for i in range(val_count)])\n",
    "val_labels = np.array([[0]*NUM_CLASSES for i in range(val_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/letter/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    val_images[index][w+h*width] = img.getpixel((w,h))\n",
    "            val_labels[index][i] = 1\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个卷积层\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, 16], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[16]), name=\"b_conv1\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 2, 2, 1]\n",
    "pool_strides = [1, 2, 2, 1]\n",
    "L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "# 第二个卷积层\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([3, 3, 16, 32], stddev=0.1), name=\"W_conv2\")\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[32]), name=\"b_conv2\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 1, 1, 1]\n",
    "pool_strides = [1, 1, 1, 1]\n",
    "L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "\n",
    "# 全连接层\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([10 * 10 * 32, 512], stddev=0.1), name=\"W_fc1\")\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[512]), name=\"b_fc1\")\n",
    "h_pool2_flat = tf.reshape(L2_pool, [-1, 10 * 10 * 32])\n",
    "h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "# readout层\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([512, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "\n",
    "# 定义优化器和训练op\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次训练迭代: train 准确率 12.10280%, val准确率 6.70391%\n",
      "第 5 次训练迭代: train 准确率 63.32554%, val准确率 28.66165%\n",
      "第 10 次训练迭代: train 准确率 82.05608%, val准确率 57.42045%\n",
      "第 15 次训练迭代: train 准确率 90.10903%, val准确率 65.38742%\n",
      "第 20 次训练迭代: train 准确率 95.97352%, val准确率 72.91718%\n",
      "第 25 次训练迭代: train 准确率 97.35981%, val准确率 75.51615%\n",
      "第 30 次训练迭代: train 准确率 98.27103%, val准确率 76.14768%\n",
      "第 35 次训练迭代: train 准确率 98.90966%, val准确率 77.11926%\n",
      "第 40 次训练迭代: train 准确率 99.05763%, val准确率 78.57664%\n",
      "第 45 次训练迭代: train 准确率 99.29128%, val准确率 79.71824%\n",
      "第 50 次训练迭代: train 准确率 99.53271%, val准确率 79.54822%\n",
      "第 55 次训练迭代: train 准确率 99.56386%, val准确率 79.40248%\n",
      "第 60 次训练迭代: train 准确率 99.81309%, val准确率 78.84382%\n",
      "第 65 次训练迭代: train 准确率 99.78193%, val准确率 79.52393%\n",
      "第 70 次训练迭代: train 准确率 99.88317%, val准确率 79.64537%\n",
      "第 75 次训练迭代: train 准确率 99.89097%, val准确率 80.15545%\n",
      "第 80 次训练迭代: train 准确率 99.73520%, val准确率 80.03401%\n",
      "第 85 次训练迭代: train 准确率 99.92990%, val准确率 79.11100%\n",
      "第 90 次训练迭代: train 准确率 99.95327%, val准确率 76.51202%\n",
      "第 95 次训练迭代: train 准确率 99.40810%, val准确率 80.00972%\n",
      "第 100 次训练迭代: train 准确率 99.96106%, val准确率 78.18800%\n",
      "第 105 次训练迭代: train 准确率 99.96884%, val准确率 77.70221%\n",
      "第 110 次训练迭代: train 准确率 99.97663%, val准确率 79.57250%\n",
      "第 115 次训练迭代: train 准确率 99.95327%, val准确率 79.08671%\n",
      "第 120 次训练迭代: train 准确率 99.96106%, val准确率 78.13942%\n",
      "第 125 次训练迭代: train 准确率 99.96106%, val准确率 79.13529%\n",
      "第 130 次训练迭代: train 准确率 99.97663%, val准确率 77.38644%\n",
      "第 135 次训练迭代: train 准确率 99.97663%, val准确率 76.68205%\n",
      "第 140 次训练迭代: train 准确率 99.97663%, val准确率 77.96940%\n",
      "第 145 次训练迭代: train 准确率 99.97663%, val准确率 77.67792%\n",
      "第 150 次训练迭代: train 准确率 99.97663%, val准确率 79.79111%\n",
      "第 155 次训练迭代: train 准确率 99.97663%, val准确率 78.79524%\n",
      "第 160 次训练迭代: train 准确率 99.97663%, val准确率 81.05416%\n",
      "第 165 次训练迭代: train 准确率 99.97663%, val准确率 81.02987%\n",
      "第 170 次训练迭代: train 准确率 99.97663%, val准确率 80.78698%\n",
      "第 175 次训练迭代: train 准确率 99.97663%, val准确率 79.91256%\n",
      "第 180 次训练迭代: train 准确率 99.96106%, val准确率 80.85985%\n",
      "第 185 次训练迭代: train 准确率 99.97663%, val准确率 80.00972%\n",
      "第 190 次训练迭代: train 准确率 99.97663%, val准确率 79.83969%\n",
      "第 195 次训练迭代: train 准确率 99.97663%, val准确率 80.49551%\n",
      "第 200 次训练迭代: train 准确率 99.97663%, val准确率 78.81953%\n",
      "第 205 次训练迭代: train 准确率 99.97663%, val准确率 79.35390%\n",
      "第 210 次训练迭代: train 准确率 99.97663%, val准确率 79.91256%\n",
      "第 215 次训练迭代: train 准确率 99.97663%, val准确率 80.81127%\n",
      "第 220 次训练迭代: train 准确率 99.97663%, val准确率 80.76269%\n",
      "第 225 次训练迭代: train 准确率 99.97663%, val准确率 81.39422%\n",
      "第 230 次训练迭代: train 准确率 99.96884%, val准确率 79.96114%\n",
      "第 235 次训练迭代: train 准确率 99.97663%, val准确率 80.17974%\n",
      "第 240 次训练迭代: train 准确率 99.97663%, val准确率 79.06243%\n",
      "第 245 次训练迭代: train 准确率 99.97663%, val准确率 79.45105%\n",
      "第 250 次训练迭代: train 准确率 99.96884%, val准确率 78.79524%\n",
      "第 255 次训练迭代: train 准确率 99.97663%, val准确率 79.66966%\n",
      "第 260 次训练迭代: train 准确率 99.97663%, val准确率 79.64537%\n",
      "第 265 次训练迭代: train 准确率 99.97663%, val准确率 79.57250%\n",
      "第 270 次训练迭代: train 准确率 99.97663%, val准确率 79.71824%\n",
      "第 275 次训练迭代: train 准确率 99.97663%, val准确率 78.74666%\n",
      "第 280 次训练迭代: train 准确率 99.97663%, val准确率 80.27690%\n",
      "第 285 次训练迭代: train 准确率 99.97663%, val准确率 79.49964%\n",
      "第 290 次训练迭代: train 准确率 99.97663%, val准确率 80.00972%\n",
      "第 295 次训练迭代: train 准确率 99.97663%, val准确率 79.01384%\n",
      "训练耗费时间：1470秒\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a3cf0233d25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mimg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mimg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    time_elapsed = time.time() - time_begin\n",
    "#     print(\"读取图片文件耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "#     print (\"一共读取了 %s 个train图像， %s 个标签\" % (input_count, input_count))\n",
    "#     print (\"一共读取了 %s 个val图像， %s 个标签\" % (val_count, val_count))\n",
    "\n",
    "#     print(\"train shape:\", (input_images.shape))\n",
    "#     print(\"val shape\", (val_images.shape))\n",
    "\n",
    "\n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 60\n",
    "    iterations = iterations\n",
    "    batches_count = int(input_count / batch_size)\n",
    "    remainder = input_count % batch_size\n",
    "#     print (\"训练数据集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count+1, batch_size, remainder))\n",
    "\n",
    "    input_images = input_images.astype(\"float32\")/255\n",
    "    val_images = val_images.astype(\"float32\")/255\n",
    "    testimg = np.reshape(input_images[0], (-1, 20*20))\n",
    "#     print(testimg)\n",
    "    testimg = np.reshape(val_images[0], (-1, 20*20))\n",
    "#     print(testimg)\n",
    "\n",
    "    # 执行训练迭代\n",
    "    for it in range(iterations):\n",
    "        # 这里的关键是要把输入数组转为np.array\n",
    "        for n in range(batches_count):\n",
    "            train_step.run(feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y_: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.5})\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size;\n",
    "            train_step.run(feed_dict={x: input_images[start_index:input_count-1], y_: input_labels[start_index:input_count-1], keep_prob: 0.5})\n",
    "\n",
    "        # 每完成五次迭代，判断准确度是否已达到100%，达到则退出迭代循环\n",
    "        iterate_accuracy = 0\n",
    "        if it%5 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: input_images, y_: input_labels, keep_prob:1.0})\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: val_images, y_: val_labels, keep_prob: 1.0})\n",
    "            print ('第 %d 次训练迭代: train 准确率 %0.5f%%, val准确率 %0.5f%%' % (it, train_accuracy*100, iterate_accuracy*100))\n",
    "            if iterate_accuracy >= 0.9999 and it >= iterations:\n",
    "                break;\n",
    "\n",
    "#     print ('完成训练!')\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print (\"训练耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "    # 保存训练结果\n",
    "    if not os.path.exists(SAVER_DIR_D):\n",
    "        print ('不存在训练数据保存目录，现在创建保存目录')\n",
    "        os.makedirs(SAVER_DIR_D)\n",
    "    # 初始化saver\n",
    "    saver = tf.train.Saver()            \n",
    "    saver_path = saver.save(sess, \"%smodel.ckpt\"%(SAVER_DIR_D))\n",
    "\n",
    "    for n in range(3,8):\n",
    "        path = \"test_images/%s.bmp\" % (n)\n",
    "        img = Image.open(path)\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "\n",
    "        img_data = [[0]*SIZE for i in range(1)]\n",
    "        for h in range(0, height):\n",
    "            for w in range(0, width):\n",
    "                if img.getpixel((w, h)) < 190:\n",
    "                    img_data[0][w+h*width] = 1\n",
    "                else:\n",
    "                    img_data[0][w+h*width] = 0\n",
    "\n",
    "        result = sess.run(conv, feed_dict = {x: np.array(img_data), keep_prob: 1.0})\n",
    "\n",
    "        max1 = 0\n",
    "        max2 = 0\n",
    "        max3 = 0\n",
    "        max1_index = 0\n",
    "        max2_index = 0\n",
    "        max3_index = 0\n",
    "        for j in range(NUM_CLASSES):\n",
    "            if result[0][j] > max1:\n",
    "                max1 = result[0][j]\n",
    "                max1_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max2) and (result[0][j]<=max1):\n",
    "                max2 = result[0][j]\n",
    "                max2_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max3) and (result[0][j]<=max2):\n",
    "                max3 = result[0][j]\n",
    "                max3_index = j\n",
    "                continue\n",
    "\n",
    "        license_num = license_num + LETTERS_DIGITS[max1_index]\n",
    "        print (\"概率：  [%s %0.2f%%]    [%s %0.2f%%]    [%s %0.2f%%]\" % (LETTERS_DIGITS[max1_index],max1*100, LETTERS_DIGITS[max2_index],max2*100, LETTERS_DIGITS[max3_index],max3*100))\n",
    "\n",
    "    print (\"车牌编号是: 【%s】\" % license_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "input_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/area/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            input_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "input_images = np.array([[0]*SIZE for i in range(input_count)])\n",
    "input_labels = np.array([[0]*NUM_CLASSES for i in range(input_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/train/area/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    input_images[index][w+h*width] = img.getpixel((w,h))\n",
    "            input_labels[index][i] = 1\n",
    "            index += 1\n",
    "\n",
    "# 第一次遍历图片目录是为了获取图片总数\n",
    "val_count = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/area/%s/' % i           # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            val_count += 1\n",
    "\n",
    "# 定义对应维数和各维长度的数组\n",
    "val_images = np.array([[0]*SIZE for i in range(val_count)])\n",
    "val_labels = np.array([[0]*NUM_CLASSES for i in range(val_count)])\n",
    "\n",
    "# 第二次遍历图片目录是为了生成图片数据和标签\n",
    "index = 0\n",
    "for i in range(0,NUM_CLASSES):\n",
    "    dir = './dataset/val/area/%s/' % i          # 这里可以改成你自己的图片目录，i为分类标签\n",
    "    for rt, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            filename = dir + filename\n",
    "            img = Image.open(filename)\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            for h in range(0, height):\n",
    "                for w in range(0, width):\n",
    "                    val_images[index][w+h*width] = img.getpixel((w, h))\n",
    "            val_labels[index][i] = 1\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个卷积层\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, 16], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[16]), name=\"b_conv1\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 2, 2, 1]\n",
    "pool_strides = [1, 2, 2, 1]\n",
    "L1_pool = conv_layer(x_image, W_conv1, b_conv1, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "# 第二个卷积层\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([3, 3, 16, 32], stddev=0.1), name=\"W_conv2\")\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[32]), name=\"b_conv2\")\n",
    "conv_strides = [1, 1, 1, 1]\n",
    "kernel_size = [1, 1, 1, 1]\n",
    "pool_strides = [1, 1, 1, 1]\n",
    "L2_pool = conv_layer(L1_pool, W_conv2, b_conv2, conv_strides, kernel_size, pool_strides, padding='SAME')\n",
    "\n",
    "\n",
    "# 全连接层\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([10 * 10 * 32, 512], stddev=0.1), name=\"W_fc1\")\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[512]), name=\"b_fc1\")\n",
    "h_pool2_flat = tf.reshape(L2_pool, [-1, 10 * 10 * 32])\n",
    "h_fc1 = full_connect(h_pool2_flat, W_fc1, b_fc1)\n",
    "\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "# readout层\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([512, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "\n",
    "# 定义优化器和训练op\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer((1e-4)).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次训练迭代: train准确率 17.87576%, val准确率 7.06663%\n",
      "第 5 次训练迭代: train准确率 66.31908%, val准确率 30.48745%\n",
      "第 10 次训练迭代: train准确率 88.44034%, val准确率 63.42660%\n",
      "第 15 次训练迭代: train准确率 95.29272%, val准确率 73.40640%\n",
      "第 20 次训练迭代: train准确率 95.79920%, val准确率 77.58870%\n",
      "第 25 次训练迭代: train准确率 96.02264%, val准确率 80.09807%\n",
      "第 30 次训练迭代: train准确率 96.40995%, val准确率 79.75194%\n",
      "第 35 次训练迭代: train准确率 97.12498%, val准确率 79.95385%\n",
      "第 40 次训练迭代: train准确率 98.06346%, val准确率 79.80964%\n",
      "第 45 次训练迭代: train准确率 98.79339%, val准确率 79.55005%\n",
      "第 50 次训练迭代: train准确率 99.29987%, val准确率 79.31930%\n",
      "第 55 次训练迭代: train准确率 99.44883%, val准确率 80.06923%\n",
      "第 60 次训练迭代: train准确率 99.67228%, val准确率 79.63657%\n",
      "第 65 次训练迭代: train准确率 98.27201%, val准确率 75.88693%\n",
      "第 70 次训练迭代: train准确率 99.85104%, val准确率 81.42486%\n",
      "第 75 次训练迭代: train准确率 99.88083%, val准确率 80.35766%\n",
      "第 80 次训练迭代: train准确率 99.65738%, val准确率 78.02134%\n",
      "第 85 次训练迭代: train准确率 99.92552%, val准确率 81.25180%\n",
      "第 90 次训练迭代: train准确率 99.86593%, val准确率 79.37698%\n",
      "第 95 次训练迭代: train准确率 99.12111%, val准确率 77.79059%\n",
      "第 100 次训练迭代: train准确率 99.95531%, val准确率 81.45370%\n",
      "第 105 次训练迭代: train准确率 99.95531%, val准确率 82.43438%\n",
      "第 110 次训练迭代: train准确率 99.44883%, val准确率 79.78079%\n",
      "第 115 次训练迭代: train准确率 99.98510%, val准确率 82.98240%\n",
      "第 120 次训练迭代: train准确率 99.98510%, val准确率 82.66513%\n",
      "第 125 次训练迭代: train准确率 99.98510%, val准确率 82.40554%\n",
      "第 130 次训练迭代: train准确率 99.89573%, val准确率 78.05019%\n",
      "第 135 次训练迭代: train准确率 99.98510%, val准确率 83.29968%\n",
      "第 140 次训练迭代: train准确率 99.98510%, val准确率 82.86703%\n",
      "第 145 次训练迭代: train准确率 100.00000%, val准确率 82.75166%\n",
      "第 150 次训练迭代: train准确率 100.00000%, val准确率 82.31901%\n",
      "第 155 次训练迭代: train准确率 100.00000%, val准确率 81.68445%\n",
      "第 160 次训练迭代: train准确率 100.00000%, val准确率 82.23248%\n",
      "第 165 次训练迭代: train准确率 100.00000%, val准确率 83.21315%\n",
      "第 170 次训练迭代: train准确率 100.00000%, val准确率 82.26132%\n",
      "第 175 次训练迭代: train准确率 97.95918%, val准确率 74.90626%\n",
      "第 180 次训练迭代: train准确率 99.98510%, val准确率 82.83819%\n",
      "第 185 次训练迭代: train准确率 100.00000%, val准确率 82.95356%\n",
      "第 190 次训练迭代: train准确率 100.00000%, val准确率 82.57860%\n",
      "第 195 次训练迭代: train准确率 100.00000%, val准确率 82.46322%\n",
      "第 200 次训练迭代: train准确率 100.00000%, val准确率 82.57860%\n",
      "第 205 次训练迭代: train准确率 99.98510%, val准确率 83.32853%\n",
      "第 210 次训练迭代: train准确率 100.00000%, val准确率 82.20363%\n",
      "第 215 次训练迭代: train准确率 100.00000%, val准确率 81.04990%\n",
      "第 220 次训练迭代: train准确率 99.98510%, val准确率 82.69397%\n",
      "第 225 次训练迭代: train准确率 100.00000%, val准确率 82.52091%\n",
      "第 230 次训练迭代: train准确率 100.00000%, val准确率 83.04009%\n",
      "第 235 次训练迭代: train准确率 100.00000%, val准确率 82.46322%\n",
      "第 240 次训练迭代: train准确率 100.00000%, val准确率 82.89587%\n",
      "第 245 次训练迭代: train准确率 100.00000%, val准确率 82.43438%\n",
      "第 250 次训练迭代: train准确率 100.00000%, val准确率 82.31901%\n",
      "第 255 次训练迭代: train准确率 100.00000%, val准确率 82.34785%\n",
      "第 260 次训练迭代: train准确率 100.00000%, val准确率 83.38621%\n",
      "第 265 次训练迭代: train准确率 100.00000%, val准确率 83.09778%\n",
      "第 270 次训练迭代: train准确率 98.48056%, val准确率 76.86761%\n",
      "第 275 次训练迭代: train准确率 100.00000%, val准确率 82.75166%\n",
      "第 280 次训练迭代: train准确率 100.00000%, val准确率 82.89587%\n",
      "第 285 次训练迭代: train准确率 100.00000%, val准确率 82.83819%\n",
      "第 290 次训练迭代: train准确率 100.00000%, val准确率 82.95356%\n",
      "第 295 次训练迭代: train准确率 100.00000%, val准确率 82.54976%\n",
      "训练耗费时间：582秒\n",
      "不存在训练数据保存目录，现在创建保存目录\n",
      "\n",
      "预测：./test_images/1510076148_824_3.bmp\n",
      "概率：  [B 87.67%]    [D 10.49%]    [R 1.53%]\n",
      "城市代号是: 【B】\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    time_elapsed = time.time() - time_begin\n",
    "#     print(\"读取图片文件耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "#     print (\"一共读取了 %s 个训练图像， %s 个标签\" % (input_count, input_count))\n",
    "#     print (\"一共读取了 %s 个验证图像， %s 个标签\" % (val_count, val_count))\n",
    "\n",
    "#     print (\"train shape:\", (input_images.shape))\n",
    "#     print (\"val shape:\", (val_images.shape))\n",
    "\n",
    "    # 设置每次训练op的输入个数和迭代次数，这里为了支持任意图片总数，定义了一个余数remainder，譬如，如果每次训练op的输入个数为60，图片总数为150张，则前面两次各输入60张，最后一次输入30张（余数30）\n",
    "    batch_size = 60\n",
    "    iterations = iterations\n",
    "    batches_count = int(input_count / batch_size)\n",
    "    remainder = input_count % batch_size\n",
    "#     print (\"训练数据集分成 %s 批, 前面每批 %s 个数据，最后一批 %s 个数据\" % (batches_count+1, batch_size, remainder))\n",
    "#     print(input_images.shape)\n",
    "#     print(val_images.shape)\n",
    "    input_images.reshape((-1, 20*20))\n",
    "    val_images.reshape((-1, 20*20))\n",
    "\n",
    "    input_images = input_images.astype(\"float32\")/255\n",
    "    val_images = val_images.astype(\"float32\")/255\n",
    "    testimg = np.reshape(input_images[0], (-1, 20*20))\n",
    "#     print(testimg)\n",
    "    testimg = np.reshape(val_images[0], (-1, 20*20))\n",
    "#     print(testimg)\n",
    "#     print(input_images.shape)\n",
    "#     print(val_images.shape)\n",
    "\n",
    "    # 执行训练迭代\n",
    "    for it in range(iterations):\n",
    "        # 这里的关键是要把输入数组转为np.array\n",
    "        for n in range(batches_count):\n",
    "            train_step.run(feed_dict={x: input_images[n*batch_size:(n+1)*batch_size], y_: input_labels[n*batch_size:(n+1)*batch_size], keep_prob: 0.5})\n",
    "        if remainder > 0:\n",
    "            start_index = batches_count * batch_size;\n",
    "            train_step.run(feed_dict={x: input_images[start_index:input_count-1], y_: input_labels[start_index:input_count-1], keep_prob: 0.5})\n",
    "\n",
    "        # 每完成五次迭代，判断准确度是否已达到100%，达到则退出迭代循环\n",
    "        iterate_accuracy = 0\n",
    "        if it%5 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: input_images, y_: input_labels, keep_prob: 1.0})\n",
    "            iterate_accuracy = accuracy.eval(feed_dict={x: val_images, y_: val_labels, keep_prob: 1.0})\n",
    "            print ('第 %d 次训练迭代: train准确率 %0.5f%%, val准确率 %0.5f%%' % (it, train_accuracy*100, iterate_accuracy*100))\n",
    "            if iterate_accuracy >= 0.9999 and it >= iterations:\n",
    "                break;\n",
    "\n",
    "#     print ('完成训练!')\n",
    "    time_elapsed = time.time() - time_begin\n",
    "    print (\"训练耗费时间：%d秒\" % time_elapsed)\n",
    "    time_begin = time.time()\n",
    "\n",
    "    # 保存训练结果\n",
    "    if not os.path.exists(SAVER_DIR_L):\n",
    "        print ('不存在训练数据保存目录，现在创建保存目录')\n",
    "        os.makedirs(SAVER_DIR_L)\n",
    "    # 初始化saver\n",
    "    saver = tf.train.Saver()            \n",
    "    saver_path = saver.save(sess, \"%smodel.ckpt\"%(SAVER_DIR_L))\n",
    "\n",
    "    for n in range(1, 2):\n",
    "        #path = \"test_images/%s.bmp\" % (n)\n",
    "        path = os.path.join(\"./test_images\", \"1510076148_824_3.bmp\")\n",
    "        print(\"\\n预测：./test_images/1510076148_824_3.bmp\")\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((20,20))\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "\n",
    "        img_data = np.array([[0]*SIZE for i in range(1)])\n",
    "        for h in range(0, height):\n",
    "            for w in range(0, width):\n",
    "                img_data[0][w+h*width] = img.getpixel((w,h))\n",
    "\n",
    "        img = img_data.astype('float32')/255\n",
    "\n",
    "        result = sess.run(conv, feed_dict = {x: img, keep_prob: 1.0})\n",
    "\n",
    "        max1 = 0\n",
    "        max2 = 0\n",
    "        max3 = 0\n",
    "        max1_index = 0\n",
    "        max2_index = 0\n",
    "        max3_index = 0\n",
    "        for j in range(NUM_CLASSES):\n",
    "            if result[0][j] > max1:\n",
    "                max1 = result[0][j]\n",
    "                max1_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max2) and (result[0][j]<=max1):\n",
    "                max2 = result[0][j]\n",
    "                max2_index = j\n",
    "                continue\n",
    "            if (result[0][j]>max3) and (result[0][j]<=max2):\n",
    "                max3 = result[0][j]\n",
    "                max3_index = j\n",
    "                continue\n",
    "\n",
    "        if n == 3:\n",
    "            license_num += \"-\"\n",
    "        license_num = license_num + LETTERS[max1_index]\n",
    "        print (\"概率：  [%s %0.2f%%]    [%s %0.2f%%]    [%s %0.2f%%]\" % (LETTERS[max1_index],max1*100, LETTERS[max2_index],max2*100, LETTERS[max3_index],max3*100))\n",
    "\n",
    "    print (\"城市代号是: 【%s】\" % license_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
